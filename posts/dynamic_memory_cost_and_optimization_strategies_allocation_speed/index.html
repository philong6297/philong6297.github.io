<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Dymanic Memory Cost and Optimization Strategies: Allocation Speed | LongLP</title><meta name=keywords content="cpp"><meta name=description content="1 Introduction When it comes to memory usage, there are (typically) two types of programs. The first type is programs that allocate memory in large blocks. Usually, they know precisely how much memory they will need and can allocate it in advance. Usually, these programs create a monolith of memory with a fixed size (e.g. holding their memory in arrays or vectors) and typically access it linearly (but only sometimes so)."><meta name=author content="longlp"><link rel=canonical href=https://philong6297.github.io/posts/dynamic_memory_cost_and_optimization_strategies_allocation_speed/><link crossorigin=anonymous href=/assets/css/stylesheet.2e47ffddd5e3f5568a7da09828f5950ef07511824dcadb34c30c775ea3549464.css integrity="sha256-Lkf/3dXj9VaKfaCYKPWVDvB1EYJNyts0wwx3XqNUlGQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.8f4558fe13d35bf64986a19e09ff0c1eb791cc6d8c81193b52bdc48d24f1f22b.js integrity="sha256-j0VY/hPTW/ZJhqGeCf8MHreRzG2MgRk7Ur3EjSTx8is=" onload=hljs.highlightAll()></script>
<link rel=icon href=https://philong6297.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://philong6297.github.io/images/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://philong6297.github.io/images/favicon-32x32.png><link rel=apple-touch-icon href=https://philong6297.github.io/images/apple-touch-icon.png><link rel=mask-icon href=https://philong6297.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Dymanic Memory Cost and Optimization Strategies: Allocation Speed"><meta property="og:description" content="1 Introduction When it comes to memory usage, there are (typically) two types of programs. The first type is programs that allocate memory in large blocks. Usually, they know precisely how much memory they will need and can allocate it in advance. Usually, these programs create a monolith of memory with a fixed size (e.g. holding their memory in arrays or vectors) and typically access it linearly (but only sometimes so)."><meta property="og:type" content="article"><meta property="og:url" content="https://philong6297.github.io/posts/dynamic_memory_cost_and_optimization_strategies_allocation_speed/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-31T00:00:00+00:00"><meta property="article:modified_time" content="2022-12-31T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dymanic Memory Cost and Optimization Strategies: Allocation Speed"><meta name=twitter:description content="1 Introduction When it comes to memory usage, there are (typically) two types of programs. The first type is programs that allocate memory in large blocks. Usually, they know precisely how much memory they will need and can allocate it in advance. Usually, these programs create a monolith of memory with a fixed size (e.g. holding their memory in arrays or vectors) and typically access it linearly (but only sometimes so)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://philong6297.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Dymanic Memory Cost and Optimization Strategies: Allocation Speed","item":"https://philong6297.github.io/posts/dynamic_memory_cost_and_optimization_strategies_allocation_speed/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Dymanic Memory Cost and Optimization Strategies: Allocation Speed","name":"Dymanic Memory Cost and Optimization Strategies: Allocation Speed","description":"1 Introduction When it comes to memory usage, there are (typically) two types of programs. The first type is programs that allocate memory in large blocks. Usually, they know precisely how much memory they will need and can allocate it in advance. Usually, these programs create a monolith of memory with a fixed size (e.g. holding their memory in arrays or vectors) and typically access it linearly (but only sometimes so).","keywords":["cpp"],"articleBody":"1 Introduction When it comes to memory usage, there are (typically) two types of programs. The first type is programs that allocate memory in large blocks. Usually, they know precisely how much memory they will need and can allocate it in advance. Usually, these programs create a monolith of memory with a fixed size (e.g. holding their memory in arrays or vectors) and typically access it linearly (but only sometimes so). These programs might even use dynamic memory, but when they do, they commonly call malloc only a few times during the program’s lifetime. In other words, memory allocation and memory organization are not typically limiting factors in these programs.\nThe second type of program, which is more well-known to us, uses memory differently. For development reasons (e.g. scalability or security), it is not straightforward to calculate how many resources a program of this type will need. Thus, they might use data structures or message passing, which require allocating a mass of small memory chunks (using malloc). Accordingly, the number of allocation/deallocation requests is much more than the former type.\nPrograms indeed need to spend time on memory allocation. However, for performance-crucial programs, it may raise a performance bottleneck issue. One should investigate if the allocation time is notably long.\nPlease note that there are two things to consider when talking about the performance of dynamic memory usage:\nMemory Allocation Speed: depends mostly on how exemplary the implementations of malloc or free are. Allocator Access Speed: depends on hardware memory subsystems. Some system allocators are better at allocating memory in a way that benefits hardware memory subsystems. This post focuses on allocation speed, demonstrates the causes of memory allocation/deallocation sluggishness, and suggests a (comprehensive) technique list on how to speed up critical places. The access speed will be the topic of a follow-up post.\n2 Drawbacks of malloc and free design Because malloc and free are parts of the C standard1, C/C++ compiler vendors have implemented them based on The C Standard Library guideline. Undoubtedly, these functions meet all the requirements for general purposes. However, when talking about performance-critical software, they are usually the first thing individual want to replace.\n2.1 Memory fragmentation For most scenes, the allocator demands large blocks of memory from the OS. From such blocks, the allocator splits out into smaller chunks to serve the requests made by programs. There are many approaches to managing these chunks out of a large block. Each algorithm differs in speed (will the allocation be fast) and memory usage.\nGiven the already-allocated green chunks, when the allocator needs to allocate a new chunk, it traverses the list from Free Blocks Start. For speed optimization, the allocator can return the first chunk of the appropriate size (first-fit algorithm). For memory consumption, the allocator can return the chunk whose size most closely matches the requested size by the calling code (best-fit algorithm). However, the allocator itself still needs an algorithm and takes time to find an appropriate memory block of a given size. Moreover, it gets more challenging to find the block over time. The reason for this is called memory fragmentation.\nGiven the following scenario, the heap consists of 5 chunks. The program allocates all of them, and after a time, it returns chunks 1, 3 and 4.\nMemory Fragmentation Illustration. Each block is 16 bytes in size. Green blocks are marked as allocated, and red blocks are available. In the above example, when the program requests a chunk of size 32 bytes (2 consecutive 16-byte chunks), the allocator would need to traverse to find the block of that size since it is available at 3 and 4. Suppose the program wants to allocate a block of size 48 bytes. In that case, the allocator will fail because it cannot find a subset of contiguous chunks. However, there are 48 bytes available in the large block (blocks 1, 3 and 4).\nTherefore, as time passes, the malloc and free functions get slower because the block of appropriate size is harder to find. Moreover, the allocation may fail for large block requests if a continuous chunk with the requested size is unavailable (even though there is enough memory in total).\nMemory fragmentation is a severe problem for long-running systems. It causes programs to become slower or run out of memory. For example, given a TV Box, the user changes a channel every 5 seconds for 48 hours. In the beginning, it took 1 second for the video to start running after changing the channel. Then after 48 hours, it took 7 seconds to do the same. It is memory fragmentation.\n2.2 Thread Synchronization For multi-threaded programs (typical for nowadays), malloc and free are required to be thread-safe2. The simplest way a C/C++ library vendor can implement them thread-safe is to introduce mutexes to protect the critical section of those functions. However, it comes with a cost: mutex locking/unlocking are expensive operations on multi-processor systems3. Synchronization can waste the speed advantage, even how quickly the allocator can find a memory block of appropriate size.\nSeveral allocators have guaranteed the performance in multi-threaded/multi-processor systems to resolve this issue. In general, the main idea is to make it synchronization-free if the memory block is exactly accessed from a single thread. For example, reserving per-thread memory or maintaining per-thread cache for recently used memory chunks.\nThe above strategy works well for most of the use cases. Nonetheless, the runtime performance can be awful when the program allocates and deallocates memory in different threads (worst-case). Furthermore, this strategy generally increases program memory usage.\n3 Program slowness caused by allocators In overview, there are three main reasons which come from allocators that slow down the program:\nMassive demand on the allocator: Enormous memory chunk requests will limit the program’s performance.\nMemory fragmentation: The more fragmented memory is, the slower malloc and free are.\nIneffective allocator implementation: Allocators by C/C++ Standard Library are for general purposes but not ideal for performance-critical programs.\nResolving any of the above causes should make the program faster (in principle). These mentioned reasons are not entirely independent of each other. For example, reducing memory fragmentation can also be fixed by decreasing the pressure on the allocator.\n4 Optimization Strategies According to Donald Knuth [5]:\n“We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%”\nOne should consider profiling and benchmarking the program to find the root cause of slowness before optimizing. The following sections present effective techniques to reduce dynamic memory consumption and fasten runtime performance. They are suggestions for fine-tuning the applications. They do not provide or propose techniques to rewrite the program more efficiently.\n4.1 Contiguous containers of pointers In C++, polymorphism can be achieved by using vectors of pointers (e.g., vector or vector","wordCount":"5035","inLanguage":"en","datePublished":"2022-12-31T00:00:00Z","dateModified":"2022-12-31T00:00:00Z","author":{"@type":"Person","name":"longlp"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://philong6297.github.io/posts/dynamic_memory_cost_and_optimization_strategies_allocation_speed/"},"publisher":{"@type":"Organization","name":"LongLP","logo":{"@type":"ImageObject","url":"https://philong6297.github.io/images/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://philong6297.github.io accesskey=h title="LongLP (Alt + H)">LongLP</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://philong6297.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://philong6297.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://philong6297.github.io>Home</a>&nbsp;»&nbsp;<a href=https://philong6297.github.io/posts/>Posts</a></div><h1 class=post-title>Dymanic Memory Cost and Optimization Strategies: Allocation Speed</h1><div class=post-meta><span title='2022-12-31 00:00:00 +0000 UTC'>December 31, 2022</span>&nbsp;·&nbsp;24 min&nbsp;·&nbsp;longlp</div><script src=/js/reorder_refs.min.fd945cc9618b50ec332e28d44d448c34f1c06746f690df382a29d4390f2f2818.js integrity="sha256-/ZRcyWGLUOwzLijUTUSMNPHAZ0b2kN84KinUOQ8vKBg="></script>
<link rel=stylesheet href=/css/custom.min.1778a9cebde69f3ae78153408588708a16bfcf81e7a7c2579eba3d38a3f3f7e5.css integrity="sha256-F3ipzr3mnzrngVNAhYhwiha/z4Hnp8JXnro9OKPz9+U=" crossorigin=anonymous media=screen></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-introduction>1 Introduction</a></li><li><a href=#2-drawbacks-of-malloc-and-free-design>2 Drawbacks of <code>malloc</code> and <code>free</code> design</a><ul><li><a href=#21-memory-fragmentation>2.1 Memory fragmentation</a></li><li><a href=#22-thread-synchronization>2.2 Thread Synchronization</a></li></ul></li><li><a href=#3-program-slowness-caused-by-allocators>3 Program slowness caused by allocators</a></li><li><a href=#4-optimization-strategies>4 Optimization Strategies</a><ul><li><a href=#41-contiguous-containers-of-pointers>4.1 Contiguous containers of pointers</a></li><li><a href=#42-custom-stl-allocator>4.2 Custom STL Allocator</a><ul><li><a href=#421-stl-allocators---per-type-allocator>4.2.1 STL Allocators - Per-Type Allocator</a></li><li><a href=#422-per-instance-allocator>4.2.2 Per-Instance Allocator</a></li><li><a href=#423-tuning-the-custom-allocator>4.2.3 Tuning the custom allocator</a></li></ul></li><li><a href=#43-memory-chunk-caching-for-producer-consumer>4.3 Memory chunk caching for producer-consumer</a></li><li><a href=#44-small-size-optimizations>4.4 Small Size Optimizations</a></li><li><a href=#45-fighting-memory-fragmentation>4.5 Fighting memory fragmentation</a></li></ul></li><li><a href=#5-system-allocators>5 System Allocators</a><ul><li><a href=#51-allocators-on-linux>5.1 Allocators on Linux</a></li><li><a href=#52-the-performance-test>5.2 The Performance Test</a></li><li><a href=#53-notes-on-using-allocators-in-the-program>5.3 Notes on using allocators in the program</a></li></ul></li><li><a href=#6-conclusion>6 Conclusion</a></li><li><a href=#7-appendix>7 Appendix</a></li><li><a href=#8-references>8 References</a></li></ul></nav></div></details></div><div class=post-content><h1 id=1-introduction>1 Introduction<a hidden class=anchor aria-hidden=true href=#1-introduction>#</a></h1><p>When it comes to memory usage, there are (typically) two types of
programs. The first type is programs that <strong>allocate memory in large
blocks</strong>. Usually, they know <strong>precisely how much memory they will need
and can allocate it in advance</strong>. Usually, these programs create a
monolith of memory with a fixed size (e.g. holding their memory in
arrays or vectors) and typically access it linearly (but only sometimes
so). These programs might even use dynamic memory, but when they do,
they commonly call <code>malloc</code> only a few times during the program’s
lifetime. In other words, memory allocation and memory organization are
not typically limiting factors in these programs.</p><p>The second type of program, which is more well-known to us, uses memory
differently. For development reasons (e.g. scalability or
security), it is not straightforward to calculate how many resources a
program of this type will need. Thus, they might use data structures or
message passing, which require allocating a mass of small memory chunks
(using <code>malloc</code>). Accordingly, the number of allocation/deallocation
requests is much more than the former type.</p><p>Programs indeed need to spend time on memory allocation. However, for
performance-crucial programs, it may raise a performance bottleneck
issue. One should investigate if the allocation time is notably long.</p><p>Please note that there are two things to consider when talking about the
performance of dynamic memory usage:</p><ul><li><strong>Memory Allocation Speed</strong>: depends mostly on how exemplary the
implementations of <code>malloc</code> or <code>free</code> are.</li><li><strong>Allocator Access Speed</strong>: depends on hardware memory subsystems.
Some system allocators are better at allocating memory in a way that
benefits hardware memory subsystems.</li></ul><p><strong>This post focuses on allocation speed, demonstrates the causes of
memory allocation/deallocation sluggishness, and suggests a
(comprehensive) technique list on how to speed up critical places</strong>. The
<strong>access speed</strong> will be the topic of a follow-up post.</p><h1 id=2-drawbacks-of-malloc-and-free-design>2 Drawbacks of <code>malloc</code> and <code>free</code> design<a hidden class=anchor aria-hidden=true href=#2-drawbacks-of-malloc-and-free-design>#</a></h1><p>Because <code>malloc</code> and <code>free</code> are parts of the C standard<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, C/C++
compiler vendors have implemented them based on The C Standard Library
guideline. Undoubtedly, these functions meet all the requirements for
general purposes. However, when talking about performance-critical
software, they are usually the first thing individual want to replace.</p><h2 id=21-memory-fragmentation>2.1 Memory fragmentation<a hidden class=anchor aria-hidden=true href=#21-memory-fragmentation>#</a></h2><p>For most scenes, the allocator demands large blocks of memory from the
OS. From such blocks, the allocator splits out into smaller chunks to
serve the requests made by programs. There are many approaches to
managing these chunks out of a large block. Each algorithm differs in
speed (will the allocation be fast) and memory usage.</p><div class=figure style=text-align:center><img src=images/allocator-arena.png alt="Given the already-allocated green chunks, when the allocator needs to allocate a new chunk, it traverses the list from Free Blocks Start. For speed optimization, the allocator can return the first chunk of the appropriate size (first-fit algorithm). For memory consumption, the allocator can return the chunk whose size most closely matches the requested size by the calling code (best-fit algorithm)." width=100%><p class=caption>Given the already-allocated green chunks, when the allocator needs to
allocate a new chunk, it traverses the list from Free Blocks Start. For
speed optimization, the allocator can return the first chunk of the
appropriate size (first-fit algorithm). For memory consumption, the
allocator can return the chunk whose size most closely matches the
requested size by the calling code (best-fit algorithm).</p></div><p>However, <strong>the allocator itself still needs an algorithm and takes time
to find an appropriate memory block of a given size</strong>. Moreover, it gets
more challenging to find the block over time. The reason for this is
called memory fragmentation.</p><p>Given the following scenario, the heap consists of 5 chunks. The program
allocates all of them, and after a time, it returns chunks 1, 3 and 4.</p><div class=figure style=text-align:center><img src=images/memory-fragmentation-illustration.png alt="Memory Fragmentation Illustration. Each block is 16 bytes in size. Green blocks are marked as allocated, and red blocks are available." width=100%><p class=caption>Memory Fragmentation Illustration. Each block is 16 bytes in size. Green
blocks are marked as allocated, and red blocks are available.</p></div><p>In the above example, when the program requests a chunk of size 32 bytes
(2 consecutive 16-byte chunks), the allocator would need to traverse to
find the block of that size since it is available at 3 and 4. Suppose
the program wants to allocate a block of size 48 bytes. In that case,
the allocator will fail because it cannot find a subset of contiguous
chunks. However, there are 48 bytes available in the large block (blocks
1, 3 and 4).</p><p>Therefore, <strong>as time passes, the <code>malloc</code> and <code>free</code> functions get
slower because the block of appropriate size is harder to find</strong>.
Moreover, the allocation may fail for large block requests if a
continuous chunk with the requested size is unavailable (even though
there is enough memory in total).</p><p><strong>Memory fragmentation is a severe problem for long-running systems</strong>.
It causes programs to become slower or run out of memory. For example,
given a TV Box, the user changes a channel every 5 seconds for 48 hours.
In the beginning, it took 1 second for the video to start running after
changing the channel. Then after 48 hours, it took 7 seconds to do the
same. It is memory fragmentation.</p><h2 id=22-thread-synchronization>2.2 Thread Synchronization<a hidden class=anchor aria-hidden=true href=#22-thread-synchronization>#</a></h2><p>For multi-threaded programs (typical for nowadays), <strong><code>malloc</code> and
<code>free</code> are required to be thread-safe</strong><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. The simplest way a C/C++
library vendor can implement them thread-safe is to introduce mutexes to
protect the critical section of those functions. However, it comes with
a cost: mutex locking/unlocking are expensive operations on
multi-processor systems<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. <strong>Synchronization can waste the speed
advantage, even how quickly the allocator can find a memory block of
appropriate size</strong>.</p><p>Several allocators have guaranteed the performance in
multi-threaded/multi-processor systems to resolve this issue. In
general, the main idea is to make it synchronization-free if the memory
block is exactly accessed from a single thread. For example, reserving
per-thread memory or maintaining per-thread cache for recently used
memory chunks.</p><p>The above strategy works well for most of the use cases. Nonetheless,
the runtime performance can be awful when the program allocates and
deallocates memory in different threads (worst-case). Furthermore, this
strategy generally increases program memory usage.</p><h1 id=3-program-slowness-caused-by-allocators>3 Program slowness caused by allocators<a hidden class=anchor aria-hidden=true href=#3-program-slowness-caused-by-allocators>#</a></h1><p>In overview, there are three main reasons which come from allocators
that slow down the program:</p><ul><li><p><strong>Massive demand on the allocator</strong>: Enormous memory chunk requests
will limit the program’s performance.</p></li><li><p><strong>Memory fragmentation</strong>: The more fragmented memory is, the slower
<code>malloc</code> and <code>free</code> are.</p></li><li><p><strong>Ineffective allocator implementation</strong>: Allocators by C/C++ Standard
Library are for general purposes but not ideal for
performance-critical programs.</p></li></ul><p>Resolving any of the above causes should make the program faster (in
principle). These mentioned reasons are not entirely independent of each
other. For example, reducing memory fragmentation can also be fixed by
decreasing the pressure on the allocator.</p><h1 id=4-optimization-strategies>4 Optimization Strategies<a hidden class=anchor aria-hidden=true href=#4-optimization-strategies>#</a></h1><p>According to Donald Knuth <a href=#ref-premature-optimization>[5]</a>:</p><blockquote><p>“We should forget about small efficiencies, say about 97% of the
time: premature optimization is the root of all evil. Yet we should
not pass up our opportunities in that critical 3%”</p></blockquote><p><strong>One should consider profiling and benchmarking the program to find the
root cause of slowness before optimizing</strong>. The following sections
present effective techniques to reduce dynamic memory consumption and
fasten runtime performance. They are suggestions for fine-tuning the
applications. They do not provide or propose techniques to rewrite the
program more efficiently.</p><h2 id=41-contiguous-containers-of-pointers>4.1 Contiguous containers of pointers<a hidden class=anchor aria-hidden=true href=#41-contiguous-containers-of-pointers>#</a></h2><p>In C++, polymorphism can be achieved by using vectors of pointers (e.g.,
<code>vector&lt;T*></code> or <code>vector&lt;unique_ptr&lt;T>></code>). However, this solution puts
massive pressure on the system allocator: <strong>creating/releasing an
element(pointer) in the vector results in a call to <code>new</code> (<code>malloc</code>)</strong>.</p><p>Large vector(s) of pointers will lead to degradation in speed
performance as the data set grows because of the demand pressure on the
allocator and memory fragmentation.</p><p><strong>An approach for this issue is using a pointer of a vector of objects
(e.g., <code>vector&lt;T>*</code> or <code>unique_ptr&lt;vector&lt;T>></code>)</strong>. This usage ensures
all the elements in a continuous block, improves data locality, and
drastically decreases the number of calls to the system allocator.</p><p>However, this solution seems verbose since it can only work for a single
type. Fortunately, several C++ polymorphism libraries have been built to
resolve real-world problems. A fitting example is <a href=https://github.com/microsoft/proxy><strong>proxy</strong></a> from
Microsoft. It is an open-source, cross-platform, single-header library
for the purpose of making runtime polymorphism easier to implement and
faster. Wang <a href=#ref-ms-proxy>[6]</a> has posted a detailed introduction on how to use
the library.</p><h2 id=42-custom-stl-allocator>4.2 Custom STL Allocator<a hidden class=anchor aria-hidden=true href=#42-custom-stl-allocator>#</a></h2><p>STL data structures like trees (e.g. <code>set</code> and <code>map</code>), hash maps
(e.g. <code>unordered_set</code> and <code>unordered_map</code>), and vectors of pointers
<strong>make many requests for memory chunks from the allocator</strong>,
increasesing memory fragmentation and, as a consequence, decreases the
performance.</p><p><strong>Fortunately, STL data structures accept a user-specified <code>Allocator</code>
as one of the template arguments</strong>. These structures call <code>allocate</code> to
request memory and <code>deallocate</code> to release the unneeded memory from the
<code>Allocator</code>. Users can specify custom <code>Allocator</code> (by implementing these
functions and satisfying the <strong>STL <code>Allocator</code> requirements</strong><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>) for
their own needs.</p><h3 id=421-stl-allocators---per-type-allocator>4.2.1 STL Allocators - Per-Type Allocator<a hidden class=anchor aria-hidden=true href=#421-stl-allocators---per-type-allocator>#</a></h3><p>According to <a href=https://en.cppreference.com/w/cpp/named_req/Allocator#Stateful_and_stateless_allocators>cppreference</a>, issued in 01/02/2023, the C++20 Standard
<a href=#ref-iso-cpp-20>[7]</a>, <a href=#ref-iec-cpp-20>[8]</a> has stated that:</p><blockquote><p>Every <code>Allocator</code> type is either <strong>stateful</strong> or <strong>stateless</strong>.
Generally, a <strong>stateful</strong> allocator type can have unequal values which
denote distinct memory resources, while a <strong>stateless</strong> allocator type
denotes a single memory resource.</p></blockquote><p>Given <code>std::allocator</code> and <code>CustomStatelessAllocator</code> are stateless
allocators, <code>CustomStatefulAllocator</code> is a stateful allocator. Consider
the following information:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;</span> <span class=n>vector_a</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>allocator</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;&gt;</span> <span class=n>vector_b</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=n>CustomStatelessAllocator</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;&gt;</span> <span class=n>vector_c</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=n>CustomStatefulAllocator</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;&gt;</span> <span class=n>vector_d</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>set</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;</span> <span class=n>set_a</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>set</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>less</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>allocator</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;&gt;</span> <span class=n>set_b</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>set</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>less</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;</span><span class=p>,</span> <span class=n>CustomStatelessAllocator</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;&gt;</span> <span class=n>set_c</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>set</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>less</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;</span><span class=p>,</span> <span class=n>CustomStatefulAllocator</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;&gt;</span> <span class=n>set_d</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>map</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=kt>bool</span><span class=o>&gt;</span> <span class=n>map_a</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>map</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=kt>bool</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>less</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=n>std</span><span class=o>::</span><span class=n>allocator</span><span class=o>&lt;</span><span class=n>std</span><span class=o>::</span><span class=n>pair</span><span class=o>&lt;</span><span class=k>const</span> <span class=kt>char</span><span class=p>,</span> <span class=o>&gt;</span> <span class=kt>bool</span><span class=o>&gt;&gt;</span>
</span></span><span class=line><span class=cl>    <span class=n>map_b</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>map</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=kt>bool</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>less</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=n>CustomStatelessAllocator</span><span class=o>&lt;</span><span class=n>std</span><span class=o>::</span><span class=n>pair</span><span class=o>&lt;</span><span class=k>const</span> <span class=kt>char</span><span class=p>,</span> <span class=kt>bool</span><span class=o>&gt;&gt;&gt;</span>
</span></span><span class=line><span class=cl>    <span class=n>map_c</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>map</span><span class=o>&lt;</span><span class=kt>char</span><span class=p>,</span> <span class=kt>bool</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>less</span><span class=o>&lt;</span><span class=kt>char</span><span class=o>&gt;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=n>CustomStatefulAllocator</span><span class=o>&lt;</span><span class=n>std</span><span class=o>::</span><span class=n>pair</span><span class=o>&lt;</span><span class=k>const</span> <span class=kt>char</span><span class=p>,</span> <span class=kt>bool</span><span class=o>&gt;&gt;&gt;</span>
</span></span><span class=line><span class=cl>    <span class=n>map_d</span><span class=p>;</span>
</span></span></code></pre></td></tr></table></div></div><table><thead><tr><th>instance</th><th>allocator</th><th>group</th></tr></thead><tbody><tr><td><code>vector_a</code></td><td><code>std::allocator&lt;char></code> (defaulted)</td><td>1</td></tr><tr><td><code>vector_b</code></td><td><code>std::allocator&lt;char></code></td><td>1</td></tr><tr><td><code>vector_c</code></td><td><code>CustomStatelessAllocator&lt;char></code></td><td>2</td></tr><tr><td><code>vector_d</code></td><td><code>CustomStatefulAllocator&lt;char></code></td><td>3</td></tr><tr><td><code>set_a</code></td><td><code>std::allocator&lt;char></code> (defaulted)</td><td>1</td></tr><tr><td><code>set_b</code></td><td><code>std::allocator&lt;char></code></td><td>1</td></tr><tr><td><code>set_c</code></td><td><code>CustomStatelessAllocator&lt;char></code></td><td>2</td></tr><tr><td><code>set_d</code></td><td><code>CustomStatefulAllocator&lt;char></code></td><td>3</td></tr><tr><td><code>map_a</code></td><td><code>std::allocator&lt;std::pair&lt;const char, bool>></code> (defaulted)</td><td>4</td></tr><tr><td><code>map_b</code></td><td><code>std::allocator&lt;std::pair&lt;const char, bool>></code></td><td>4</td></tr><tr><td><code>map_c</code></td><td><code>CustomStatelessAllocator&lt;std::pair&lt;const char, bool>></code></td><td>5</td></tr><tr><td><code>map_d</code></td><td><code>CustomStatefulAllocator&lt;std::pair&lt;const char, bool>></code></td><td>6</td></tr></tbody></table><p>From the results, it is evident that <strong><code>std::allocator</code> is per-type
allocators</strong>: all the data structures of the same type share one
instance of the allocator.</p><h4 id=4211-design-overview>4.2.1.1 Design Overview<a hidden class=anchor aria-hidden=true href=#4211-design-overview>#</a></h4><p>In a regular allocator, memory chunks belonging to different data
structures may end up next to one another in the memory. With an STL
allocator, data from different domains are guaranteed in separate memory
blocks. This design leads to several improvements:</p><ul><li><strong>Since data from the same data structure is in the same memory block,
it not only decreases memory fragmentation but also increases data
locality and access speed:</strong><ul><li>STL allocator guarantees that memory chunks are returned to the
allocator when the data structure is destroyed. Additionally, when
all data structures of the same type are destroyed, the STL
allocator returns the free memory to the system.</li><li>Consecutive calls to <code>allocate</code> and <code>deallocate</code> guarantee to return
neighboring chunks in memory</li></ul></li><li><strong>Simple implementation opens up speed opportunities</strong>:<ul><li><strong>The allocator return only chunks of a (or multiple of) constant
size</strong>: memory-fragmentation-free. Any free element will be a
perfect fit for any request.
Also, since the size is already known (constant size), there is no
need for chunk metadata that keeps the information about the
chunk’s size.</li><li><strong>No thread-safe requirements</strong> unless the data structure is
allocated from several threads.</li></ul></li></ul><h4 id=4212-example-of-customizing-stl-allocator>4.2.1.2 Example of customizing STL allocator<a hidden class=anchor aria-hidden=true href=#4212-example-of-customizing-stl-allocator>#</a></h4><p><strong>Arena Allocator</strong><sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> is commonly used for data structures with little
changes after creation. Since deallocation is a no-op, destruction can
be significantly fast.</p><p>This section provides a naive <strong>Arena Allocator</strong> implementation for
<code>std::map</code>.</p><p>Given the definition of C++20 <code>std::map</code> <a href=#ref-iec-cpp-20>[8]</a>, issued in
01/02/2023 from <a href=https://en.cppreference.com/w/cpp/container/map>cppreference</a>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>template</span><span class=o>&lt;</span>
</span></span><span class=line><span class=cl>    <span class=k>class</span> <span class=nc>Key</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=k>class</span> <span class=nc>T</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=k>class</span> <span class=nc>Compare</span> <span class=o>=</span> <span class=n>std</span><span class=o>::</span><span class=n>less</span><span class=o>&lt;</span><span class=n>Key</span><span class=o>&gt;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=k>class</span> <span class=nc>Allocator</span> <span class=o>=</span> <span class=n>std</span><span class=o>::</span><span class=n>allocator</span><span class=o>&lt;</span><span class=n>std</span><span class=o>::</span><span class=n>pair</span><span class=o>&lt;</span><span class=k>const</span> <span class=n>Key</span><span class=p>,</span> <span class=n>T</span><span class=o>&gt;&gt;</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=k>class</span> <span class=nc>map</span><span class=p>;</span>
</span></span></code></pre></td></tr></table></div></div><p>There is a template parameter named <code>Allocator</code>, and it defaults to
<code>std::allocator</code> (line 5). One can replace the default allocator by
writing a class with a few methods that can replace the
<code>std::allocator</code>.</p><p>Suppose a context uses <code>std::map</code> to look up a considerable number of
elements, then destroy it as soon as afterwards. One can provide a
custom allocator that allocates from one specific block. Follow the
below example:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span><span class=k>typename</span> <span class=n>T</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>arena_allocator</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=n>arena_allocator</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>memory_</span> <span class=o>=</span> <span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=n>T</span><span class=o>*&gt;</span><span class=p>(</span><span class=n>mmap</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>kMemorySize</span><span class=p>,</span> <span class=n>PROT_READ</span> <span class=o>|</span> <span class=n>PROT_WRITE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                         <span class=n>MAP_PRIVATE</span> <span class=o>|</span> <span class=n>MAP_ANONYMOUS</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=n>free_block_index_</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=o>~</span><span class=n>arena_allocator</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>munmap</span><span class=p>(</span><span class=n>memory_</span><span class=p>,</span> <span class=n>kMemorySize</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// ....
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>  <span class=na>[[nodiscard]]</span> <span class=k>constexpr</span> <span class=n>T</span><span class=o>*</span> <span class=n>allocate</span><span class=p>(</span><span class=n>std</span><span class=o>::</span><span class=n>size_t</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>T</span> <span class=o>*</span><span class=n>result</span> <span class=o>=</span> <span class=o>&amp;</span><span class=n>memory_</span><span class=p>[</span><span class=n>free_block_index_</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>free_block_index_</span> <span class=o>+=</span> <span class=n>n</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>result</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>constexpr</span> <span class=kt>void</span> <span class=nf>deallocate</span><span class=p>(</span><span class=n>T</span><span class=o>*</span> <span class=n>p</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>size_t</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// deallocate everything when destroyed
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>private</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=n>T</span><span class=o>*</span> <span class=n>memory_</span><span class=p>{</span><span class=k>nullptr</span><span class=p>};</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>free_block_index_</span><span class=p>{</span><span class=mi>0</span><span class=p>};</span>
</span></span><span class=line><span class=cl>  <span class=k>static</span> <span class=k>constexpr</span> <span class=kt>int</span> <span class=n>kMemorySize</span><span class=p>{</span><span class=mi>1000</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span><span class=p>};</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// using the custom allocator
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>std</span><span class=o>::</span><span class=n>map</span><span class=o>&lt;</span><span class=kt>int</span><span class=p>,</span> <span class=n>my_class</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>less</span><span class=o>&lt;</span><span class=kt>int</span><span class=o>&gt;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=n>arena_allocator</span><span class=o>&lt;</span><span class=n>std</span><span class=o>::</span><span class=n>pair</span><span class=o>&lt;</span><span class=k>const</span> <span class=kt>int</span><span class=p>,</span> <span class=n>my_class</span><span class=o>&gt;&gt;&gt;</span>
</span></span><span class=line><span class=cl>    <span class=n>my_ds</span><span class=p>;</span>
</span></span></code></pre></td></tr></table></div></div><p>On line 5, call <code>mmap</code> to allocate a large memory block from the OS (it
will not allocate all that RAM unless the program uses it). When the
<code>arena_allocator</code> instance is destroyed (line 11), the block will get
returned to the system. Method <code>allocate</code> returns the block’s
first available chunk (lines 17-19), and method <code>deallocate</code> does
nothing. Therefore, the allocation is fast, and deallocating is a no-op.</p><p>This approach is ideal for the scene where allocating a complete
<code>std::map</code> at the beginning and then does not do any removal operations.
The tree (<code>std::map</code> is usually implemented as a red-black tree) will be
compact in memory, which is suiteable for the cache-hit rate and
performance. It will make no memory fragmentation since the whole block
is separated from other memory blocks.</p><p>When removing elements from <code>std::map</code>, even though <code>std::map</code> would
call <code>deallocate</code>, no memory would be released. The program’s
memory consumption would go up. If this is the case, one might need to
implement the <code>deallocate</code> method, but allocation also needs to become
more complex.</p><h3 id=422-per-instance-allocator>4.2.2 Per-Instance Allocator<a hidden class=anchor aria-hidden=true href=#422-per-instance-allocator>#</a></h3><p><strong>Per-Instance Allocator</strong> is ideal when having several large data
structures of the same type. By using it, the memory needed for a
particular data structure instance will be allocated from a dedicated
block instead of separating memory blocks per domain (allocating a
memory pool for each type).</p><p>Take an example of <code>std::map&lt;student_id, student></code>, given having two
instances, one for undergraduate students and the other for graduate
students. With standard STL allocators, both hash maps share the same
allocator. With a per-instance allocator, when destroying an instance,
the whole memory block used for that instance is empty and can be
directly returned to the OS. This benefits memory fragmentation
reduction and data locality increment (faster map traversal).</p><p><strong>Unfortunately, STL data structures do not support per-instance
allocators. Thus, rewriting the data structure with
per-instance-allocator support is indeed a need</strong>.</p><h3 id=423-tuning-the-custom-allocator>4.2.3 Tuning the custom allocator<a hidden class=anchor aria-hidden=true href=#423-tuning-the-custom-allocator>#</a></h3><p><strong>A custom allocator can be adjusted to a specific environment for
maximum performance</strong>. The following approaches can be taken into
account when choosing the right strategy for an individual’s
custom allocator:</p><ul><li><strong>Static allocator</strong>: Preloading with enough space to store elements
can be perfectly fit for small data structures. If the context
requires extra memory, the allocator can request it from the system
allocator. This approach can substantially decrease the number of
calls to the system allocator.</li><li><strong>Arena allocator</strong>: Releasing memory only when the allocator is
destroyed. It is helpful for large data structures that are primarily
static after creation.</li><li><strong>Cache allocator</strong>: When deallocating, keep specific chunks in the
cache instead of returning the whole memory to the system allocator
for future usage of allocating requests. Despite not decreasing memory
fragmentation, this approach can slow down fragmenting progression.</li></ul><h2 id=43-memory-chunk-caching-for-producer-consumer>4.3 Memory chunk caching for producer-consumer<a hidden class=anchor aria-hidden=true href=#43-memory-chunk-caching-for-producer-consumer>#</a></h2><p>Take an example where a producer thread allocates an object and sends it
to the consumer thread. After processing, the consumer thread destroys
the object and releases the memory back to the OS. <strong>This context puts
significant pressure on the system allocator. One approach is to
allocate all the objects from a dedicated memory pool using a custom
allocator.</strong></p><p>In C++, one could overload the <code>operator new/delete</code> to use the new
allocator. However, synchronisation must be aware since the object
memory lifetime is controlled in separate threads. One idea is to use a
memory cache on both the allocation and deallocation places. Consider
the following source code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span><span class=k>typename</span> <span class=n>T</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>memory_pool</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=n>T</span><span class=o>*</span> <span class=n>allocate</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>allocation_cache_</span><span class=p>.</span><span class=n>empty</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=kt>int</span> <span class=n>move_count</span><span class=p>{};</span>
</span></span><span class=line><span class=cl>      <span class=kt>int</span> <span class=n>remaining_count</span><span class=p>{};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=n>common_cache_mutex_</span><span class=p>.</span><span class=n>lock</span><span class=p>();</span>
</span></span><span class=line><span class=cl>      <span class=n>move_count</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>          <span class=n>allocation_cache_</span><span class=p>.</span><span class=n>move_to</span><span class=p>(</span><span class=n>common_cache_</span><span class=p>,</span> <span class=n>allocation_cache_</span><span class=p>.</span><span class=n>capacity</span><span class=p>());</span>
</span></span><span class=line><span class=cl>      <span class=n>common_cache_mutex_</span><span class=p>.</span><span class=n>unlock</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=n>remaining_count</span> <span class=o>=</span> <span class=n>allocation_cache_</span><span class=p>.</span><span class=n>capacity</span><span class=p>()</span> <span class=o>-</span> <span class=n>move_count</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=k>for</span> <span class=p>(</span><span class=k>auto</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>remaining_count</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>allocation_cache_</span><span class=p>.</span><span class=n>push_front</span><span class=p>(</span><span class=n>malloc</span><span class=p>(</span><span class=k>sizeof</span><span class=p>(</span><span class=n>T</span><span class=p>)));</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>allocation_cache_</span><span class=p>.</span><span class=n>pop_front</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kt>void</span> <span class=nf>deallocate</span><span class=p>(</span><span class=n>T</span><span class=o>*</span> <span class=n>p</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>deallocation_cache_</span><span class=p>.</span><span class=n>full</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=kt>int</span> <span class=n>remaining_count</span><span class=p>{};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=n>common_cache_mutex_</span><span class=p>.</span><span class=n>lock</span><span class=p>();</span>
</span></span><span class=line><span class=cl>      <span class=n>common_cache_</span><span class=p>.</span><span class=n>move_to</span><span class=p>(</span><span class=n>deallocation_cache_</span><span class=p>,</span> <span class=n>deallocation_cache_</span><span class=p>.</span><span class=n>capacity</span><span class=p>());</span>
</span></span><span class=line><span class=cl>      <span class=n>common_cache_mutex_</span><span class=p>.</span><span class=n>unlock</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=n>remaining_count</span> <span class=o>=</span> <span class=n>deallocation_cache_</span><span class=p>.</span><span class=n>count</span><span class=p>();</span>
</span></span><span class=line><span class=cl>      <span class=k>for</span> <span class=p>(</span><span class=k>auto</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>remaining_count</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>free</span><span class=p>(</span><span class=n>deallocation_cache_</span><span class=p>.</span><span class=n>pop_front</span><span class=p>());</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>deallocation_cache_</span><span class=p>.</span><span class=n>push_front</span><span class=p>(</span><span class=n>p</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>private</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=n>chunk_list</span><span class=o>&lt;</span><span class=n>T</span><span class=o>*&gt;</span> <span class=n>allocation_cache_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>chunk_list</span><span class=o>&lt;</span><span class=n>T</span><span class=o>*&gt;</span> <span class=n>deallocation_cache_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>chunk_list</span><span class=o>&lt;</span><span class=n>T</span><span class=o>*&gt;</span> <span class=n>common_cache_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>std</span><span class=o>::</span><span class=n>mutex</span> <span class=n>common_cache_mutex_</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Usage
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>object</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=kt>void</span><span class=o>*</span> <span class=k>operator</span> <span class=k>new</span><span class=p>(</span><span class=n>size_t</span> <span class=n>size</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>m_pool</span><span class=p>.</span><span class=n>allocate</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kt>void</span> <span class=k>operator</span> <span class=nf>delete</span><span class=p>(</span><span class=kt>void</span><span class=o>*</span> <span class=n>p</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>m_pool</span><span class=p>.</span><span class=n>deallocate</span><span class=p>(</span><span class=n>p</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=k>private</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=k>static</span> <span class=n>memory_pool</span><span class=o>&lt;</span><span class=n>object</span><span class=o>&gt;</span> <span class=n>m_pool</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div><p>Inside the custom allocator <code>memory_pool</code>are three linked lists
containing the cached memory chunks: <code>allocation_cache_</code>,
<code>common_cache_</code> and <code>deallocation_cache_</code>.</p><ul><li><strong>When <code>deallocate</code> is called, the memory chunk is not released back
to the system allocator. Instead, it is cached</strong> (line 37).<ul><li>If <code>deallocation_cache_</code> is full, memory chunks from it are moved to
<code>common_cache_</code>(line 28). When the <code>common_cache_</code> becomes full, the
remaining memory chunks are released back to the system allocator
(lines 32-34). <code>deallocation_cache_</code> is empty after this operation.</li><li>Access to <code>common_cache_</code> has to be secured with a mutex (lines 27
and 29).</li><li>Access to <code>deallocation_cache_</code> is the typical case.<code>memory_pool</code>
only need to access <code>common_cache_</code> when <code>deallocation_cache_</code> is
full.</li></ul></li><li><strong>When <code>allocate</code> is called, if available, a memory chunk is taken
from the cache</strong> (line 20).<ul><li>If <code>allocation_cache_</code> is empty, additional memory chunks are taken
from the <code>common_cache_</code>(line 10). The <code>memory_pool</code> will move
chunks (as much as possible) from the <code>common_cache_</code> to the
<code>allocation_cache_</code>until <code>allocation_cache_</code> is full.</li><li>If the <code>allocation_cache_</code> is not filled after this operation, the
system allocator will request additional memory chunks (lines
15-17).</li><li>Access to <code>common_cache_</code> has to be secured with a mutex (lines 9
and 12).</li><li>Access to <code>allocation_cache_</code> is the regular case. <code>memory_pool</code>
only need to access <code>common_cache_</code> when <code>allocation_cache_</code> is
empty.</li></ul></li></ul><p>This solution works only with two threads, one allocates, and the other
deallocates objects.</p><p>For optimally, the size of <code>allocation_cache_</code>, <code>deallocation_cache_</code>
and <code>common_cache_</code> need to be chosen carefully:</p><ul><li>Using small values forces the <code>memory_pool</code> to work more with the
system allocator than the cached chunk lists.</li><li>Using large values causes the <code>memory_pool</code> to consume more memory.</li><li>An ideal size for <code>common_cache_</code> is about two times bigger than the
capacity of the others.</li></ul><h2 id=44-small-size-optimizations>4.4 Small Size Optimizations<a hidden class=anchor aria-hidden=true href=#44-small-size-optimizations>#</a></h2><p>Given the class <code>small_vector</code> for storing integers with the following
(naive) implementation:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>class</span> <span class=nc>small_vector</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=n>small_vector</span><span class=p>(</span><span class=n>size_t</span> <span class=n>capacity</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>size_</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>capacity_</span> <span class=o>=</span> <span class=n>capacity</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>data_</span> <span class=o>=</span> <span class=n>malloc</span><span class=p>(</span><span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>)</span> <span class=o>*</span> <span class=n>capacity</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span><span class=o>&amp;</span> <span class=k>operator</span><span class=p>[](</span><span class=n>size_t</span> <span class=n>index</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>data_</span><span class=p>[</span><span class=n>index</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>private</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span><span class=o>*</span> <span class=n>data_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>size_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>capacity_</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div><p>Suppose the profiling report shows that in most of the runtime cases,
these objects have small sizes of up to 4. <strong>A solution could be
pre-allocating four integers, thus, totally reducing calls to the system
allocator</strong>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>class</span> <span class=nc>small_vector</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=n>small_vector</span><span class=p>(</span><span class=n>size_t</span> <span class=n>capacity</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>size_</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>capacity</span> <span class=o>&lt;=</span> <span class=n>kPreAllocatedSize</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>capacity_</span> <span class=o>=</span> <span class=n>kPreAllocatedSize</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=n>data_</span> <span class=o>=</span> <span class=n>pre_allocated_storage_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>capacity_</span> <span class=o>=</span> <span class=n>capacity</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=n>data_</span> <span class=o>=</span> <span class=n>malloc</span><span class=p>(</span><span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>)</span> <span class=o>*</span> <span class=n>capacity</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>private</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>static</span> <span class=k>constexpr</span> <span class=kt>int</span> <span class=n>kPreAllocatedSize</span> <span class=o>=</span> <span class=mi>4</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>pre_allocated_storage_</span><span class=p>[</span><span class=n>kPreAllocatedSize</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div><p>The above codes illustrate creating a <code>small_vector</code> with a capacity
less than or equal to <code>kPreAllocatedSize</code> (lines 6-7) for ordinary use
cases by using <code>pre_allocated_storage_</code>. Otherwise, call the system
allocator as the origin implementation (lines 9-10).</p><p>However, the drawback is the class size increment. On a 64-bit system,
the original was 24 bytes, while the new is 40 bytes.</p><p><strong>A common solution is using C unions to overlay the data for the
pre-allocated case and the heap-allocated case</strong>. The most significant
bit in <code>size_</code> can be used for authorization between these cases (<code>0</code>
for pre-allocated and <code>1</code> for heap-allocated).</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>class</span> <span class=nc>small_vector</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=n>small_vector</span><span class=p>(</span><span class=n>size_t</span> <span class=n>capacity</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>capacity</span> <span class=o>&gt;</span> <span class=n>kPreAllocatedSize</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>data_</span><span class=p>.</span><span class=n>heap_storage_</span><span class=p>.</span><span class=n>capacity_</span> <span class=o>=</span> <span class=n>capacity</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=n>data_</span><span class=p>.</span><span class=n>heap_storage_</span><span class=p>.</span><span class=n>data_</span> <span class=o>=</span> <span class=p>(</span><span class=kt>int</span><span class=o>*</span><span class=p>)</span><span class=n>malloc</span><span class=p>(</span><span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>)</span> <span class=o>*</span> <span class=n>capacity</span><span class=p>);</span>
</span></span><span class=line><span class=cl>      <span class=n>size_</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>|</span> <span class=n>kHeadSizeMask</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>size_</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kt>bool</span> <span class=nf>is_pre_allocated</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>size_</span> <span class=o>&amp;</span> <span class=n>kHeadSizeMask</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kt>int</span><span class=o>&amp;</span> <span class=k>operator</span><span class=p>[](</span><span class=n>size_t</span> <span class=n>index</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>is_pre_allocated</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>return</span> <span class=n>data_</span><span class=p>.</span><span class=n>pre_allocated_storage_</span><span class=p>[</span><span class=n>index</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>data_</span><span class=p>.</span><span class=n>heap_storage_</span><span class=p>.</span><span class=n>data_</span><span class=p>[</span><span class=n>index</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=nf>size</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>size_</span> <span class=o>&amp;</span> <span class=p>(</span><span class=o>~</span><span class=n>kHeadSizeMask</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>private</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=k>static</span> <span class=k>constexpr</span> <span class=kt>int</span> <span class=n>kPreAllocatedSize</span> <span class=o>=</span> <span class=mi>4</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>static</span> <span class=k>constexpr</span> <span class=n>size_t</span> <span class=n>kHeadSizeMask</span> <span class=o>=</span> <span class=mi>1ULL</span> <span class=o>&lt;&lt;</span> <span class=p>(</span><span class=k>sizeof</span><span class=p>(</span><span class=n>size_t</span><span class=p>)</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>-</span> <span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=k>union</span> <span class=nc>data_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=kt>int</span><span class=o>*</span> <span class=n>data_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=n>size_t</span> <span class=n>capacity_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=n>heap_storage_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>pre_allocated_storage_</span><span class=p>[</span><span class=n>kPreAllocatedSize</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>};</span>
</span></span><span class=line><span class=cl>  <span class=n>data_t</span> <span class=n>data_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>size_</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></td></tr></table></div></div><p>This approach is used in several places. For example, <strong>libc++
<code>std::string</code> implementation</strong><sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</p><h2 id=45-fighting-memory-fragmentation>4.5 Fighting memory fragmentation<a hidden class=anchor aria-hidden=true href=#45-fighting-memory-fragmentation>#</a></h2><p>It is always challenging to reduce memory fragmentation or make it never
happens. Sometimes, it is caused by the system. There are a few
approaches that can help in these situations:</p><ul><li><p><strong>Restart</strong>: some systems will occasionally restart to avoid memory
fragmentation. However, creating a restartable or state-restorable
program or system can be challenging.</p></li><li><p><strong>Pre-allocate memory upfront</strong>: Some programs pre-allocate all the
needed memory at the start and then completely dispense with dynamic
memory allocation. MISRA coding guidelines even forbids the usage of
dynamic memory in the automotive industry:</p><blockquote><p>MISRA C++ 2008 <a href=#ref-misra-cpp-2008>[11]</a>, 18-4-1 - Dynamic heap memory allocation
shall not be used.</p></blockquote><blockquote><p>MISRA C:2004 <a href=#ref-misra-c-2004>[12]</a>, 20.4 - Dynamic heap memory allocation
shall not be used.</p></blockquote><blockquote><p>MISRA C:2012 <a href=#ref-misra-c-2012>[13]</a>, 21.3 The memory allocation and
deallocation functions of &lt;stdlib.h> shall not be used</p></blockquote><p>However, these guidelines are impossible for programs in other
industries where the needed memory is unknown at the beginning of the
program.</p></li><li><p><strong>Changing the system allocator</strong>: several system allocators can be
used instead of the built-in one and which promise faster allocation
speed, better memory usage, less fragmentation, or better data
locality.</p></li></ul><h1 id=5-system-allocators>5 System Allocators<a hidden class=anchor aria-hidden=true href=#5-system-allocators>#</a></h1><p>This section describes a solution to speed up programs by using a better
system allocator for individual needs. Several open-source allocators
try to achieve efficient allocation and deallocation. Despite that,
there has yet to be any allocator who has taken the holy grail.</p><p>Regularly, there are four perspectives that each allocator compromises
on:</p><ul><li><strong>Allocation Speed</strong>: Note that both the speed of <code>malloc</code> and <code>free</code>
(<code>new</code> and <code>delete</code> in C++) are essential.</li><li><strong>Memory Consumption</strong>: Percentage of wasted memory after allocating.
The allocator needs to keep some accounting info for each block, which
generally takes up some space. Additionally, if the allocator
optimizes for allocation speed, it can leave some memory unused.</li><li><strong>Memory Fragmentation</strong>: Some allocators have these issues than
others, which can affect the speed of long-running applications.</li><li><strong>Cache/Data Locality</strong>: Allocators which pack data in smaller blocks
and avoid memory losses have better cache/data locality (which will be
discussed in a follow-up article)</li></ul><h2 id=51-allocators-on-linux>5.1 Allocators on Linux<a hidden class=anchor aria-hidden=true href=#51-allocators-on-linux>#</a></h2><p>When one does not specify any configurations, the Linux programs use
<strong>GNU Allocator</strong>, based on <a href=http://www.malloc.de/en/><strong>ptmalloc</strong></a>. Apart from it, there are
several other open-source allocators commonly used on Linux:
<a href=https://github.com/google/tcmalloc><strong>tcmalloc</strong></a> (by Google), <a href=https://github.com/jemalloc/jemalloc><strong>jemalloc</strong></a> (by Facebook),
<a href=https://github.com/microsoft/mimalloc><strong>mimalloc</strong></a> (by Microsoft) and <a href=https://github.com/emeryberger/Hoard><strong>hoard allocator</strong></a>.</p><p>GNU Allocator is not among the most efficient allocators. However, it
does have one advantage, the worst-case runtime and memory usage will be
all right.</p><p>Other allocators claim to be better in speed, memory usage, or
cache/data locality. Before choosing a new system allocator, consider
thinking about the following questions:</p><ul><li>Single-thread or multi-threaded?</li><li>Maximum allocation speed or minimum memory consumption? What is the
acceptable trade-off?</li><li>Is the allocator for the whole program or only the most critical
parts?</li></ul><p>In Linux, after installing the allocator, one can use an environment
variable <code>LD_PRELOAD</code> to replace the default allocator with a custom
one:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ <span class=nv>LD_PRELOAD</span><span class=o>=</span>/usr/lib/x86_64-Linux-gnu/libtcmalloc_minimal.so.4
</span></span><span class=line><span class=cl>./my_program
</span></span></code></pre></td></tr></table></div></div><h2 id=52-the-performance-test>5.2 The Performance Test<a hidden class=anchor aria-hidden=true href=#52-the-performance-test>#</a></h2><p>Real-world programs differ too much from one another. Therefore,
providing a comprehensive benchmark for allocator performance is
impossible in this article’s scope. An allocator performing well
under one load can have different behaviour under another.</p><p>“No Bugs” Hare <a href=#ref-it-hare-testing-allocators>[14]</a> has compared several allocators
on a test load that tries to simulate a real-world load. According to
the author’s conclusion:</p><blockquote><p>allocators are similar, and testing your particular application is
more important than relying on synthetic benchmarks.</p></blockquote><h2 id=53-notes-on-using-allocators-in-the-program>5.3 Notes on using allocators in the program<a hidden class=anchor aria-hidden=true href=#53-notes-on-using-allocators-in-the-program>#</a></h2><p><strong>All the allocators can be fine-tuned to run better on a particular
system</strong>, but the default configuration should be enough for most use
cases. Fine-tuning can be done through environment variables,
configuration files, or compilation options.</p><p>Usually, the allocators provide implementations for <code>malloc</code> and <code>free</code>,
and replace the functions with the same name provided by the Standard C
Library. This design means the program’s dynamic allocation goes
through the new allocator.</p><p>However, it is possible to keep default <code>malloc</code> and <code>free</code>
implementations separately with custom implementations provided by the
chosen allocator. Allocators can provide prefixed versions of <code>malloc</code>
and <code>free</code> for this purpose (e.g., <strong>jemalloc</strong> with the prefix <code>je_</code>).
In this case, <code>malloc</code> and <code>free</code> will be left unchanged, and one can
use <code>je_malloc</code> and <code>je_free</code> to allocate memory only for some parts of
the program through <code>jemalloc</code>.</p><h1 id=6-conclusion>6 Conclusion<a hidden class=anchor aria-hidden=true href=#6-conclusion>#</a></h1><p>This article represented several optimization solutions for how to
allocate memory faster.</p><p><strong>Ready-made allocators have the benefit of being relatively easy to set
up, and one can see the improvements within minutes</strong></p><p><strong>Other battle-tested techniques are also powerful when used
correctly</strong>. For example, decreasing allocation requests by avoiding
pointers removes much stress from the system allocator; using custom
allocators can benefit allocation speed and decrease memory
fragmentation.</p><p>Lastly, people need to <strong>know their domain and profile the program to
find the root cause of slowness before optimizing, then try multiple
approaches</strong>. Repeating this process is indeed improving the
program’s quality.</p><h1 id=7-appendix>7 Appendix<a hidden class=anchor aria-hidden=true href=#7-appendix>#</a></h1><h1 id=8-references>8 References<a hidden class=anchor aria-hidden=true href=#8-references>#</a></h1><div id=refs class="references csl-bib-body"><div id=ref-iso-c-17 class=csl-entry><p><span class=csl-left-margin>[1]
</span><span class=csl-right-inline>“<span class=nocase>Information
technology - Programming languages - C</span>,” International
Organization for Standardization; ISO/IEC 9899:2018, International
Standard, Jun. 2018.Available:
<a href=https://www.iso.org/standard/74528.html>https://www.iso.org/standard/74528.html</a></span></p></div><div id=ref-iec-c-17 class=csl-entry><p><span class=csl-left-margin>[2]
</span><span class=csl-right-inline>“<span class=nocase>Information
technology - Programming languages - C</span>,” International
Electrotechnical Commission; ISO/IEC 9899:2018, International Standard,
Jun. 2018.Available: <a href=https://webstore.iec.ch/publication/63478>https://webstore.iec.ch/publication/63478</a></span></p></div><div id=ref-mutex-lock-cost-explain class=csl-entry><p><span class=csl-left-margin>[3]
</span><span class=csl-right-inline>Dummy00001,
“<span class=nocase>How efficient is locking and unlocked mutex?
What is the cost of a mutex?</span>” stackoverflow, 2010
[Online].Available: <a href=https://stackoverflow.com/a/3652428>https://stackoverflow.com/a/3652428</a></span></p></div><div id=ref-mutex-lock-cost-benchmark class=csl-entry><p><span class=csl-left-margin>[4]
</span><span class=csl-right-inline>C. Wood, “How efficient is
locking and unlocked mutex? What is the cost of a mutex?”
stackoverflow, 2019 [Online].Available:
<a href=https://stackoverflow.com/a/49712993>https://stackoverflow.com/a/49712993</a></span></p></div><div id=ref-premature-optimization class=csl-entry><p><span class=csl-left-margin>[5]
</span><span class=csl-right-inline>D. E. Knuth, “Structured
programming with go to statements,” <em>ACM Comput. Surv.</em>, vol. 6,
no. 4, pp. 261–301, Dec. 1974, doi:
<a href=https://doi.org/10.1145/356635.356640>10.1145/356635.356640</a>.</span></p></div><div id=ref-ms-proxy class=csl-entry><p><span class=csl-left-margin>[6]
</span><span class=csl-right-inline>M. Wang, “Proxy: Runtime
polymorphism made easier than ever.” Microsoft, 2022
[Online].Available:
<a href=https://devblogs.microsoft.com/cppblog/proxy-runtime-polymorphism-made-easier-than-ever/>https://devblogs.microsoft.com/cppblog/proxy-runtime-polymorphism-made-easier-than-ever/</a></span></p></div><div id=ref-iso-cpp-20 class=csl-entry><p><span class=csl-left-margin>[7]
</span><span class=csl-right-inline>“<span class=nocase>Programming
languages - C++</span>,” International Organization for
Standardization; ISO/IEC 14882:2020, International Standard, Dec.
2020.Available: <a href=https://www.iso.org/standard/79358.html>https://www.iso.org/standard/79358.html</a></span></p></div><div id=ref-iec-cpp-20 class=csl-entry><p><span class=csl-left-margin>[8]
</span><span class=csl-right-inline>“<span class=nocase>Programming
languages - C++</span>,” International Electrotechnical
Commission; ISO/IEC 14882:2020, International Standard, Dec.
2020.Available: <a href=https://webstore.iec.ch/publication/68285>https://webstore.iec.ch/publication/68285</a></span></p></div><div id=ref-region-based-memory-management class=csl-entry><p><span class=csl-left-margin>[9]
</span><span class=csl-right-inline>M. Tofte and J.-P. Talpin,
“Region-based memory management,” <em>Information and
Computation</em>, vol. 132, no. 2, pp. 109–176, 1997, doi:
<a href=https://doi.org/10.1006/inco.1996.2613>https://doi.org/10.1006/inco.1996.2613</a>.</span></p></div><div id=ref-libcpp-implementation class=csl-entry><p><span class=csl-left-margin>[10]
</span><span class=csl-right-inline>J. Laity,
“<span class=nocase>libc++’s implementation of
std::string</span>.” joellaity, blog, 2020 [Online].Available:
<a href=https://joellaity.com/2020/01/31/string.html>https://joellaity.com/2020/01/31/string.html</a></span></p></div><div id=ref-misra-cpp-2008 class=csl-entry><p><span class=csl-left-margin>[11]
</span><span class=csl-right-inline>Motor Industry Software
Reliability Association, <em><span class=nocase>MISRA-C++:2008:
Guidelines for the Use of the C++ Language in Critical Systems</span></em>.
MIRA Limited, 2008.Available:
<a href="https://books.google.com.vn/books?id=bNUqPQAACAAJ">https://books.google.com.vn/books?id=bNUqPQAACAAJ</a></span></p></div><div id=ref-misra-c-2004 class=csl-entry><p><span class=csl-left-margin>[12]
</span><span class=csl-right-inline>Motor Industry Software
Reliability Association, <em><span class=nocase>MISRA-C:2004: Guidelines
for the Use of the C Language in Critical Systems</span></em>. MIRA,
2004.Available:
<a href="https://books.google.com.vn/books?id=j6oXAAAACAAJ">https://books.google.com.vn/books?id=j6oXAAAACAAJ</a></span></p></div><div id=ref-misra-c-2012 class=csl-entry><p><span class=csl-left-margin>[13]
</span><span class=csl-right-inline>Motor Industry Software
Reliability Association and Motor Industry Software Reliability
Association Staff and HORIBA MIRA Ltd and HORIBA MIRA Ltd. Staff,
<em><span class=nocase>MISRA C:2012: Guidelines for the Use of the C
Language in Critical Systems</span></em>. Unknown Publisher, 2019.Available:
<a href="https://books.google.com.vn/books?id=daApxQEACAAJ">https://books.google.com.vn/books?id=daApxQEACAAJ</a></span></p></div><div id=ref-it-hare-testing-allocators class=csl-entry><p><span class=csl-left-margin>[14]
</span><span class=csl-right-inline>“No Bugs” Hare,
“<span class=nocase>Testing Memory Allocators: ptmalloc2 vs
tcmalloc vs hoard vs jemalloc While Trying to Simulate Real-World
Loads</span>.” IT Hare on Soft.ware, blog, 2018
[Online].Available:
<a href=http://ithare.com/testing-memory-allocators-ptmalloc2-tcmalloc-hoard-jemalloc-while-trying-to-simulate-real-world-loads/>http://ithare.com/testing-memory-allocators-ptmalloc2-tcmalloc-hoard-jemalloc-while-trying-to-simulate-real-world-loads/</a></span></p></div></div><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>At the time of writing, C17 standard <a href=#ref-iso-c-17>[1]</a>, <a href=#ref-iec-c-17>[2]</a> has
stated the definitions of <code>malloc</code> and <code>free</code> functions:</p><ul><li>7.22.3.4 The malloc function (p: 254)</li><li>7.22.3.3 The free function (p: 254)</li></ul>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:2><p>According to the C17 standard <a href=#ref-iso-c-17>[1]</a>, <a href=#ref-iec-c-17>[2]</a>:</p><blockquote><p><code>free</code> is thread-safe: it behaves as though only accessing the
memory locations visible through its argument, and not any static
storage.
A call to <code>free</code> that deallocates a region of memory
<strong>synchronizes-with</strong> a call to any subsequent allocation function
that allocates the same or a part of the same region of memory.
This synchronization occurs after any access to the memory by the
deallocating function and before any access to the memory by the
allocation function. There is a single total order of all
allocation and deallocation functions operating on each particular
region of memory.</p></blockquote><blockquote><p><code>malloc</code> is thread-safe: it behaves as though only accessing the
memory locations visible through its argument, and not any static
storage.
A previous call to <code>free</code> or <code>realloc</code> that deallocates a region
of memory synchronizes-with a call to malloc that allocates the
same or a part of the same region of memory. This synchronization
occurs after any access to the memory by the deallocating function
and before any access to the memory by malloc. There is a single
total order of all allocation and deallocation functions operating
on each particular region of memory.</p></blockquote>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:3><p>Stackoverflow user “Dummy00001” has explained that the
primary overhead of mutexes is from the memory/cache coherency
guarantees <a href=#ref-mutex-lock-cost-explain>[3]</a>. In the same article, the user “Carlo
Wood” has also benchmarked the number of clocks it takes to
lock/unlock mutex on a decent multi-core machine and gave favorable
results with “Dummy00001”’s statement<a href=#ref-mutex-lock-cost-benchmark>[4]</a>.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>A summary of C++20 Standard <code>Allocator</code> requirements <a href=#ref-iso-cpp-20>[7]</a>,
<a href=#ref-iec-cpp-20>[8]</a> can be found at <a href=https://en.cppreference.com/w/cpp/named_req/Allocator#Requirements>cppreference</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><strong>Region-based memory management</strong><a href=#ref-region-based-memory-management>[9]</a> is a type of memory
management in which each allocated object is assigned to a region.
The region (also known as the arena) is a collection of allocated
objects that can be efficiently reallocated or deallocated all at
once. Like stack allocation, regions facilitate the allocation and
deallocation of memory with low overhead. However, they are more
flexible, allowing objects to live longer than the stack frame in
which they were allocated. In typical implementations, all objects
in a region are allocated in a single contiguous range of memory
addresses, similar to how stack frames are typically allocated.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>The implementation of <strong>libc++ <code>std::string</code></strong> can be found at
their <a href=https://github.com/llvm/llvm-project/blob/main/libcxx/include/string>repository</a>. Unfortunately, the source code is hard to read
and undocumented. Laity <a href=#ref-libcpp-implementation>[10]</a> has posted an overview of their
design and optimisation for this implementation&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><hr><footer class=post-footer><ul class=post-tags><li><a href=https://philong6297.github.io/tags/cpp/>cpp</a></li></ul><nav class=paginav><a class=prev href=https://philong6297.github.io/posts/cpp_parallel_stl_is_not_yet_for_indiscriminate_use/><span class=title>« Prev</span><br><span>C++ Parallel STL is not yet for indiscriminate use</span></a>
<a class=next href=https://philong6297.github.io/posts/about_inlining/><span class=title>Next »</span><br><span>About inlining</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 Phi-Long Le</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>