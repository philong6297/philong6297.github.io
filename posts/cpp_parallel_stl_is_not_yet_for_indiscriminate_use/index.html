<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>C++ Parallel STL is not yet for indiscriminate use | LongLP</title><meta name=keywords content="cpp"><meta name=description content="1 Abstract As of today, the hardware usually comes with multi-core architectures. People need to rely on something other than hardware vendors to improve single-core performance.
The hardware free-lunch has been over for about 15 years.
— Herb Sutter [1] —
In order to follow this evolution, one needs to ensure that software is performance-compatible with multi-core machines. The software industry started with a trend of incorporating concurrency in action. As expected, ISO C++ has also started providing high-level abstractions for expressing parallelism, moving beyond simple threads and synchronization primitives: In 2017, the C++ standard introduced the so-called parallel algorithms."><meta name=author content="longlp"><link rel=canonical href=https://philong6297.github.io/posts/cpp_parallel_stl_is_not_yet_for_indiscriminate_use/><link crossorigin=anonymous href=/assets/css/stylesheet.2e47ffddd5e3f5568a7da09828f5950ef07511824dcadb34c30c775ea3549464.css integrity="sha256-Lkf/3dXj9VaKfaCYKPWVDvB1EYJNyts0wwx3XqNUlGQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.8f4558fe13d35bf64986a19e09ff0c1eb791cc6d8c81193b52bdc48d24f1f22b.js integrity="sha256-j0VY/hPTW/ZJhqGeCf8MHreRzG2MgRk7Ur3EjSTx8is=" onload=hljs.highlightAll()></script>
<link rel=icon href=https://philong6297.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://philong6297.github.io/images/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://philong6297.github.io/images/favicon-32x32.png><link rel=apple-touch-icon href=https://philong6297.github.io/images/apple-touch-icon.png><link rel=mask-icon href=https://philong6297.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="C++ Parallel STL is not yet for indiscriminate use"><meta property="og:description" content="1 Abstract As of today, the hardware usually comes with multi-core architectures. People need to rely on something other than hardware vendors to improve single-core performance.
The hardware free-lunch has been over for about 15 years.
— Herb Sutter [1] —
In order to follow this evolution, one needs to ensure that software is performance-compatible with multi-core machines. The software industry started with a trend of incorporating concurrency in action. As expected, ISO C++ has also started providing high-level abstractions for expressing parallelism, moving beyond simple threads and synchronization primitives: In 2017, the C++ standard introduced the so-called parallel algorithms."><meta property="og:type" content="article"><meta property="og:url" content="https://philong6297.github.io/posts/cpp_parallel_stl_is_not_yet_for_indiscriminate_use/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-20T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-20T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="C++ Parallel STL is not yet for indiscriminate use"><meta name=twitter:description content="1 Abstract As of today, the hardware usually comes with multi-core architectures. People need to rely on something other than hardware vendors to improve single-core performance.
The hardware free-lunch has been over for about 15 years.
— Herb Sutter [1] —
In order to follow this evolution, one needs to ensure that software is performance-compatible with multi-core machines. The software industry started with a trend of incorporating concurrency in action. As expected, ISO C++ has also started providing high-level abstractions for expressing parallelism, moving beyond simple threads and synchronization primitives: In 2017, the C++ standard introduced the so-called parallel algorithms."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://philong6297.github.io/posts/"},{"@type":"ListItem","position":3,"name":"C++ Parallel STL is not yet for indiscriminate use","item":"https://philong6297.github.io/posts/cpp_parallel_stl_is_not_yet_for_indiscriminate_use/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"C++ Parallel STL is not yet for indiscriminate use","name":"C\u002b\u002b Parallel STL is not yet for indiscriminate use","description":"1 Abstract As of today, the hardware usually comes with multi-core architectures. People need to rely on something other than hardware vendors to improve single-core performance.\nThe hardware free-lunch has been over for about 15 years.\n— Herb Sutter [1] —\nIn order to follow this evolution, one needs to ensure that software is performance-compatible with multi-core machines. The software industry started with a trend of incorporating concurrency in action. As expected, ISO C++ has also started providing high-level abstractions for expressing parallelism, moving beyond simple threads and synchronization primitives: In 2017, the C++ standard introduced the so-called parallel algorithms.","keywords":["cpp"],"articleBody":"1 Abstract As of today, the hardware usually comes with multi-core architectures. People need to rely on something other than hardware vendors to improve single-core performance.\nThe hardware free-lunch has been over for about 15 years.\n— Herb Sutter [1] —\nIn order to follow this evolution, one needs to ensure that software is performance-compatible with multi-core machines. The software industry started with a trend of incorporating concurrency in action. As expected, ISO C++ has also started providing high-level abstractions for expressing parallelism, moving beyond simple threads and synchronization primitives: In 2017, the C++ standard introduced the so-called parallel algorithms. In essence, this feature offers parallel versions of the existing STL algorithms.\nThis article aims to demonstrate the author’s subjective opinion on the current status of C++ Parallel Algorithms 1 (as they were introduced in C++17 and are currently present in C++20). Although adding parallel versions to some STL algorithms is a reasonable improvement, the author disputes that this is not such a significant advancement as one might think.\n2 Introduction Given a context where there is a need to parallelize the following transform algorithm:\n1 2 3 4 std::transform(in.begin(), in.end(), // input range out.begin(), // output range ftor // transform fun ); Since C++ 17, one could follow the std::transform interface and use it as below:\n1 2 3 4 5 std::transform(std::execution::par_unseq, // parallel policy in.begin(), in.end(), // input range out.begin(), // output range ftor // transform fun ); The difference versus the traditional invocation of transform is the first parameter for setting up parallel policy hints. This parameter tells the algorithm to use parallelization and vectorization in the given case.\nThe official term for this param is Execution Policy 2 3. It suggests the type of execution for the algorithm. As of current ISO C++ (C++ 23), there are four parallel policies:\nseq\nAn instance of std::sequenced_policy. It uses the sequential version of the algorithm. The “classic” version of STL algorithms (no Execution Policy param) is also supposed to be the same as this one. par\nAn instance of std::parallel_policy. The algorithm is allowed to parallelize but disallowed to vectorize. par_unseq\nInstance of std::parallel_unsequenced_policy. The algorithm can be both parallelized and vectorized. unseq\nInstance of std::unsequenced_policy. The algorithm is allowed to vectorize but disallowed to parallelize. Therefore, the parallelizing effort is minimal. One could specify the parallel policy to transform an existing STL algorithm into a parallel (or vectorized) version.\nHowever, STL algorithm implementations only considered the Execution Policy as a hint or the maximum parallelization/vectorization level allowed. They can ignore it and fall back to the serial execution entirely.\nCurrently, most STL algorithms can take the Execution Policy parameter as an instruction to run in parallel. Several new algorithms were added to overcome the constraints of existing algorithms that forbid parallelizing them or that there are better ways to express some parallel algorithms (reduce, exclusive_scan, inclusive_scan, transform_reduce, transform_exclusive_scan, transform_inclusive_scane).\nThis article primarily focuses on parallel execution (par policy), which aims to utilize all the available cores to increase efficiency. Nonetheless, the author briefly touches on vectorization (unseq policy) towards the end of the article 4.\n3 Difficulty in current C++ Parallel Algorithms design 3.1 No Concurrency awareness The first thing to notice is that it is straightforward to adapt existing algorithms and make them parallel. This improvement explains the success of parallel algorithms (at least at the perception level).\nDespite the C++ committee not intending to solve the concurrency problem with STL Parallel (they wanted to solve local efficiency problems), it also has a negative effect from a didactical point of view 5. C++ parallel algorithms have not allowed a global concurrency design. It allows only local optimizations by making some algorithm calls parallelizable. It is reasonable, for limited domains, to focus more on efficiency than design, but that is typically different with concurrency. One must pay attention to the design to get suboptimal efficiency. In other words, multi-core efficiency is a global optimization problem, not a local one. Considering this, the C++ users might misunderstand that they do not need to pay attention to concurrency issues; One can count on C++ STL to magically resolve them.\nThis problem is concluded to be the same problem that initially led people to insufficient concurrency design. Instead of recognizing that concurrency needs an entirely new type of design, the C++ standard tried to apply sequential thinking by adding the ability to run on multiple threads 6. Dropping the old ways of writing software and adopting a new paradigm is necessary for proper concurrency.\n3.2 Not yet completely production-ready the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used\n– Martin Reddy [11] –\nBased on Amdahl’s law [12], [13], one needs to have a significant part of the parallelizable code to have speed improvements from parallelism. In the case of Parallel STL, one needs to have considerable parts of the application using such algorithms. In other words, the vast majority of the time must be spent on STL algorithms to have relevant performance benefits.\nHowever, not every program consists just of STL algorithm calls. Currently, many programs have flows that are tough to reduce to STL algorithms (if it is even possible). These applications can only expose control flow concurrency, making them unsuitable for using parallel algorithms. Imagine a context where the application is doing graph processing. In most cases, no STL algorithms are ready yet to be used.\n3.3 Combining STL algorithms introduces sequential behavior Suppose a context where one needs to call multiple STL algorithms. If the algorithms were to interact, then they need to be called serially:\nSTL Algorithms Flow Illustration. In the above figure, three flow parts are bound to execute sequentially: before the first algorithm, between the algorithm calls, and at the end of the second algorithm. Moreover, no matter how exemplary the implementation is, the synchronizing and starting tasks need additional time. Thus, according to Amdahl’s law [12], [13], this puts an upper limit on performance improvement.\nNevertheless, whenever the algorithm ends, it must wait for all the threads to finish executing, which reduces the machine’s parallelism capacity. This behavior results from the fact that the work cannot perfectly distribute between threads; several threads process more than others.\nTo counter this problem, one could do one or more of the following:\nStart algorithms from multiple threads Ensure that algorithms have continuations or pipelining, avoiding serially calling algorithms Tune the parallel algorithms to ensure the work’s dividing. The first item can be possible outside parallel STL without direct support. However, the current design of the C++ standard library does not allow one to implement any of the latter.\n3.4 Small datasets are not suitable for parallelization To make it worthwhile to parallelize an algorithm, the execution time of the algorithm needs to be significant 7. Teodorescu [14] has refocused Amdahl’s Law [12], [13] and suggested that the algorithm needs to take more than 100 milliseconds to be beneficial from parallelizing. Other optimization opportunities could be better helpful if this is not the case.\nSuch algorithms (which have long execution times) either have a sufficiently large number of elements (e.g., millions of elements) in the collection, or the operation given to the algorithm needs to take a long time. While the latter can be prevalent in many applications, the former (considerable elements) is typically unpopular.\nTo give an order of magnitude for numerous elements, Bartlomiej [5] benchmarked two experiments on a decent high-performance machine. The first benchmark is a simple transform operation (the functor doubling the value received). For 100k elements, the execution times decrease from 0.524 to 0.389 seconds. A 1.3x improvement for six cores machine is not worth parallelizing. After this test, Bartlomiej writes [5]:\nAs you see on the faster machine, you need like 1 million elements to start seeing some performance gains. On the other hand, on my notebook, all parallel implementations were slower.\nThe last experiment is a real-world problem: Computing the Fresnel transformation on 3D vectors. The parallelization improvement result is sufficient, an approximate 6x improvement (from 1.697 seconds to 0.283 seconds) for 100k elements. Although these tests are biased, they still present a rough observation: One needs a significant number of elements (100k) for the performance improvement to be hundreds of milliseconds.\nConsider other benchmarks from the parallel algorithms implementor of Microsoft Visual Studio’s standard library, Billy [6]. His article illustrates several benchmarks for sorting algorithms. The experiment contexts use 1 million elements, resulting in less than 100 milliseconds.\nTo conclude, to see significant performance improvements from parallelizing STL algorithms, containers with a rough order of magnitude of 1 million elements must be made. This condition only happens sometimes.\n3.5 STL Algorithms is not tunning-friendly This problem affects all the STL algorithms. Thus, the author takes the sort algorithm as a running example. For a few elements, sorting is faster with a linear algorithm than with a parallel one. Production-ready algorithms typically have a threshold: if the number of elements is below this point, the algorithm calls the serial version. The cutoff point varies on algorithm implementations. Intel OneTBB’s parallel_sort and GNU libstdc++ set this value to 500 [15]:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 //! Sorts the data in [begin,end) using the given comparator /** The compare function object is used for all comparisons between elements during sorting. The compare object must define a bool operator() function. @ingroup algorithms **/ template\u003ctypename RandomAccessIterator, typename Compare\u003e __TBB_requires(std::random_access_iterator\u003cRandomAccessIterator\u003e \u0026\u0026 compare\u003cCompare, RandomAccessIterator\u003e \u0026\u0026 std::movable\u003citer_value_type\u003cRandomAccessIterator\u003e\u003e) void parallel_sort( RandomAccessIterator begin, RandomAccessIterator end, const Compare\u0026 comp ) { constexpr int min_parallel_size = 500; if( end \u003e begin ) { if( end - begin \u003c min_parallel_size ) { std::sort(begin, end, comp); } else { parallel_quick_sort(begin, end, comp); } } } The value 500 is well-analyzed with primitive type experiments (string, integers, floating-points) by STL vendors before being set as the threshold. However, one size does not fit all; this cutoff point must be different to sort some complex objects. Unfortunately, the C++ standard only aims to support the general purpose and does not allow any customizing of the algorithms. This leads to suboptimal performance.\nSimilarly, given a simple for_each operation, if the algorithm needs to call a heavy function, it is reasonable to have each element mapped into a task. However, if the transformation is simple (i.e., an arithmetic operation), creating one task per element is not ideal for performance. Algorithms may need tuning to adjust different input patterns.\nSimilarly, let’s assume a simple for_each operation. For some cases, if the algorithm needs to call a heavy function, it’s ok to have each element mapped into a task. On the other hand, if the transformation is simple (i.e., an arithmetic operation) then, creating one task per element will be bad for performance. Thus, algorithms may need tuning to accommodate different input patterns.\nConcore library [16] showing how one can set hints to parallel algorithms:\n1 2 3 4 5 concore::partition_hints hints; hints.granularity_ = 100; // cannot process less than 100 elements in // a single task concore::conc_for(input.begin(), input.end(), functor, hints); 3.6 Compile time overhead C++ STL is heavily templatized and needs to be implemented in header files. Even so, implementing parallel algorithms is far more complicated than serial versions and adds significant compile-time overhead 8.\nPeople could not take this problem lightly since it takes more time to compile C++ programs. Especially when applying Test-Driven-Development in the project, compilation cycles take forever to compile.\nFortunately, there are a few tricks that both implementers can employ to save part of this problem. Jorg introduced several techniques to reduce compile time costs in CppCon 2019 [18]. For example, GCC will not compile the parallel versions of the library if is not included [17].\n4 Conclusion C++ parallel STL brings parallelization and vectorization to standard algorithms without requiring much effort from the user. However, the author considered this feature under-expected and needs better support.\nFirstly, moving to multi-core programming requires a global concurrency design. C++ parallel algorithms do not offer any support for this. Moreover, the standard (accidentally) misguide to C++ users that parallelism can be easily achieved without concurrent understanding.\nSecondly, besides the high-level concurrency design issue, the standard parallel library also has limitations at the low level. As applications have more than just algorithms, using standard C++ parallel algorithms is insufficient to achieve good speedups for most applications. The article covers several problems related to lower-level performance issues:\nSTL algorithms are serialized. Typically, Small datasets are unsuitable for parallelization Impossibility of tuning the algorithms. Compile time overhead. It is true that it’s easy for a user to quickly change the policy of the STL algorithms and maybe get some performance benefits. But my focus in this article is on the empty half of the glass. I’m arguing that the benefits are not as big as one could obtain with a proper concurrency design. In some limited cases (i.e., many elements, or functors that are too complex) one might get some speedups for one’s algorithms. But even in these cases, the costs of spiking into using multiple cores may have an overall negative performance costs.\nAll these make C++ parallel algorithms less of a great addition to the concurrency frameworks. The author agrees that they are needed, but it is still insufficient. The basic executor concept would have been a better complement to the standard. Unfortunately, the executors did not make it to C++ 23. So, let people wait to see what the future will reserve.\n5 Appendix 6 References [1] H. Sutter, “The free lunch is over a fundamental turn toward concurrency in software,” 2005.\n[2] “Programming languages - C++,” International Organization for Standardization; ISO/IEC 14882:2020, International Standard, Dec. 2020.Available: https://www.iso.org/standard/79358.html\n[3] B. A. Lelbach, “C++ Parallel Algorithms and Beyond.” CppCon 2016 conference, 2016.Available: https://www.youtube.com/watch?v=UJrmee7o68A\n[4] B. Filipek, “C++17 in details: Parallel Algorithms.” C++ Stories, blog, 2017 [Online].Available: https://www.cppstories.com/2017/08/cpp17-details-parallel/\n[5] B. Filipek, “The Amazing Performance of C++17 Parallel Algorithms, is it Possible?” C++ Stories, blog, 2018 [Online].Available: https://www.cppstories.com/2018/11/parallel-alg-perf/\n[6] B. O’Neal, “Using C++17 Parallel Algorithms for Better Performance.” C++ Team Blog, blog, 2018 [Online].Available: https://devblogs.microsoft.com/cppblog/using-c17-parallel-algorithms-for-better-performance/\n[7] S. Parent, “Better Code: Concurrency.” code::dive 2016 conference, 2016.Available: https://www.youtube.com/watch?v=QIHy8pXbneI\n[8] R. Pike, “Concurrency Is Not Parallelism.” Heroku’s Waza conference, 2012.Available: https://go.dev/talks/2012/waza.slide#2\n[9] L. R. Teodorescu, “Concurrency Design Patterns,” Overload, vol. 159, pp. 12–18, 2020 [Online],Available: https://accu.org/journals/overload/28/159/overload159.pdf#page=14\n[10] K. Henney, “Thinking Outside the Synchronisation Quadrant.” ACCU 2017 conference, 2017.Available: https://www.youtube.com/watch?v=UJrmee7o68A\n[11] M. Reddy, “Chapter 7 - Performance,” in API Design for C++, M. Reddy, Ed. Boston: Morgan Kaufmann, 2011, pp. 209–240. doi: https://doi.org/10.1016/B978-0-12-385003-4.00007-5.\n[12] G. M. Amdahl, “Validity of the single processor approach to achieving large scale computing capabilities,” in Proceedings of the april 18-20, 1967, spring joint computer conference, 1967, pp. 483–485. doi: 10.1145/1465482.1465560.\n[13] D. P. Rodgers, “Improvements in multiprocessor system design,” SIGARCH Comput. Archit. News, vol. 13, no. 3, pp. 225–231, Jun. 1985, doi: 10.1145/327070.327215.\n[14] L. R. Teodorescu, “Refocusing Amdahl’s Law,” Overload, vol. 157, pp. 5–10, 2020 [Online],Available: https://accu.org/journals/overload/28/157/overload157.pdf#page=7\n[15] “oneAPI Threading Building Blocks (oneTBB),” GitHub repository. Intel Corporation; https://github.com/oneapi-src/oneTBB/blob/master/include/oneapi/tbb/parallel_sort.h; GitHub, 2023 [Online].\n[16] L. R. Teodorescu, Concore library. ver. 0.5, 2021 [Online].Available: https://github.com/lucteo/concore\n[17] T. Rodgers, “Bringing C++ 17 Parallel Algorithms to a Standard Library Near You.” CppCon 2018 conference, 2018.Available: https://www.youtube.com/watch?v=-KT8gaojHUU\n[18] J. Brown, “Reducing Template Compilation Overhead, Using C++11, 14, 17, and 20.” CppCon 2019 conference, 2019.Available: https://www.youtube.com/watch?v=TyiiNVA1syk\nThe author intercharges C++ Parallel STL term with C++ parallel algorithms. ↩︎\nAccording to ISO C++ 20 [2]:\nstd::sequenced_policy The execution policy type used as a unique type to disambiguate parallel algorithm overloading and require that a parallel algorithm’s execution may not be parallelized. The invocations of element access functions in parallel algorithms invoked with this policy (usually specified as std::execution::seq) are indeterminately sequenced in the calling thread.\nstd::parallel_policy The execution policy type used as a unique type to disambiguate parallel algorithm overloading and indicate that a parallel algorithm’s execution may be parallelized. The invocations of element access functions in parallel algorithms invoked with this policy (usually specified as std::execution::par) are permitted to execute in either the invoking thread or in a thread implicitly created by the library to support parallel algorithm execution. Any such invocations executing in the same thread are indeterminately sequenced with respect to each other.\nstd::parallel_unsequenced_policy The execution policy type used as a unique type to disambiguate parallel algorithm overloading and indicate that a parallel algorithm’s execution may be parallelized, vectorized, or migrated across threads (such as by a parent-stealing scheduler). The invocations of element access functions in parallel algorithms invoked with this policy are permitted to execute in an unordered fashion in unspecified threads, and unsequenced with respect to one another within each thread.\nstd::unsequenced_policy The execution policy type used as a unique type to disambiguate parallel algorithm overloading and indicate that a parallel algorithm’s execution may be vectorized, e.g., executed on a single thread using instructions that operate on multiple data items.\n↩︎ [3]–[6] provided comprehensive introduction and explanation of C++ parallel algorithms. ↩︎\nIn order to efficiently use vectorization without global parallelism design, one typically must focus on the local computations. The local-focus approach is perfect for applying vectorization at the STL algorithms level: It can unlock a more significant portion of the computation power available on modern hardware [7]. ↩︎\nThe author discusses parallelism, not concurrency. The distinction is well-explained by Pike and Teodorescu [8], [9]. In short, Concurrency is a design concern, while parallelism is a run-time efficiency concern. ↩︎\nThis article supports Henney’s well-discussion on the synchronization quadrant [10]. ↩︎\nPlease excuse my over-generalisation, but it doesn’t seem too rewarding to try to parallelize a 100 millisecond algorithm. Obtaining a linear speedup on a 6-core machine would save 83.3 milliseconds on the completion time, and will fully utilise all the cores.\n– Lucian Radu Teodorescu [14] –\n↩︎ Rodgers provided some metrics about this problem in CppCon 2018 [17] ↩︎\n","wordCount":"2989","inLanguage":"en","datePublished":"2023-02-20T00:00:00Z","dateModified":"2023-02-20T00:00:00Z","author":{"@type":"Person","name":"longlp"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://philong6297.github.io/posts/cpp_parallel_stl_is_not_yet_for_indiscriminate_use/"},"publisher":{"@type":"Organization","name":"LongLP","logo":{"@type":"ImageObject","url":"https://philong6297.github.io/images/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://philong6297.github.io accesskey=h title="LongLP (Alt + H)">LongLP</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://philong6297.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://philong6297.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://philong6297.github.io>Home</a>&nbsp;»&nbsp;<a href=https://philong6297.github.io/posts/>Posts</a></div><h1 class=post-title>C++ Parallel STL is not yet for indiscriminate use</h1><div class=post-meta><span title='2023-02-20 00:00:00 +0000 UTC'>February 20, 2023</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;longlp</div><script src=/js/reorder_refs.min.fd945cc9618b50ec332e28d44d448c34f1c06746f690df382a29d4390f2f2818.js integrity="sha256-/ZRcyWGLUOwzLijUTUSMNPHAZ0b2kN84KinUOQ8vKBg="></script>
<link rel=stylesheet href=/css/custom.min.1778a9cebde69f3ae78153408588708a16bfcf81e7a7c2579eba3d38a3f3f7e5.css integrity="sha256-F3ipzr3mnzrngVNAhYhwiha/z4Hnp8JXnro9OKPz9+U=" crossorigin=anonymous media=screen></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-abstract>1 Abstract</a></li><li><a href=#2-introduction>2 Introduction</a></li><li><a href=#3-difficulty-in-current-c-parallel-algorithms-design>3 Difficulty in current C++ Parallel Algorithms design</a><ul><li><a href=#31-no-concurrency-awareness>3.1 No Concurrency awareness</a></li><li><a href=#32-not-yet-completely-production-ready>3.2 Not yet completely production-ready</a></li><li><a href=#33-combining-stl-algorithms-introduces-sequential-behavior>3.3 Combining STL algorithms introduces sequential behavior</a></li><li><a href=#34-small-datasets-are-not-suitable-for-parallelization>3.4 Small datasets are not suitable for parallelization</a></li><li><a href=#35-stl-algorithms-is-not-tunning-friendly>3.5 STL Algorithms is not tunning-friendly</a></li><li><a href=#36-compile-time-overhead>3.6 Compile time overhead</a></li></ul></li><li><a href=#4-conclusion>4 Conclusion</a></li><li><a href=#5-appendix>5 Appendix</a></li><li><a href=#6-references>6 References</a></li></ul></nav></div></details></div><div class=post-content><h1 id=1-abstract>1 Abstract<a hidden class=anchor aria-hidden=true href=#1-abstract>#</a></h1><p>As of today, the hardware usually comes with multi-core architectures.
People need to rely on something other than hardware vendors to improve
single-core performance.</p><blockquote><p>The hardware free-lunch has been over for about 15 years.</p><p>— Herb Sutter <a href=#ref-Sutter05>[1]</a> —</p></blockquote><p>In order to follow this evolution, one needs to ensure that software is
performance-compatible with multi-core machines. The software industry
started with a trend of incorporating concurrency in action. As
expected, ISO C++ has also started providing high-level abstractions for
expressing parallelism, moving beyond simple threads and synchronization
primitives: In 2017, the C++ standard introduced the so-called parallel
algorithms. In essence, this feature offers parallel versions of the
existing STL algorithms.</p><p>This article aims to demonstrate the author’s subjective opinion
on the current status of <em><strong>C++ Parallel Algorithms</strong></em> <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> (as they
were introduced in C++17 and are currently present in C++20). Although
adding parallel versions to some STL algorithms is a reasonable
improvement, the author disputes that this is not such a significant
advancement as one might think.</p><h1 id=2-introduction>2 Introduction<a hidden class=anchor aria-hidden=true href=#2-introduction>#</a></h1><p>Given a context where there is a need to parallelize the following
<em><strong>transform</strong></em> algorithm:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>transform</span><span class=p>(</span><span class=n>in</span><span class=p>.</span><span class=n>begin</span><span class=p>(),</span> <span class=n>in</span><span class=p>.</span><span class=n>end</span><span class=p>(),</span> <span class=c1>// input range
</span></span></span><span class=line><span class=cl><span class=c1></span>               <span class=n>out</span><span class=p>.</span><span class=n>begin</span><span class=p>(),</span>          <span class=c1>// output range
</span></span></span><span class=line><span class=cl><span class=c1></span>               <span class=n>ftor</span>                  <span class=c1>// transform fun
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><p>Since C++ 17, one could follow the <code>std::transform</code> interface and use it
as below:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>transform</span><span class=p>(</span><span class=n>std</span><span class=o>::</span><span class=n>execution</span><span class=o>::</span><span class=n>par_unseq</span><span class=p>,</span> <span class=c1>// parallel policy
</span></span></span><span class=line><span class=cl><span class=c1></span>               <span class=n>in</span><span class=p>.</span><span class=n>begin</span><span class=p>(),</span> <span class=n>in</span><span class=p>.</span><span class=n>end</span><span class=p>(),</span>      <span class=c1>// input range
</span></span></span><span class=line><span class=cl><span class=c1></span>               <span class=n>out</span><span class=p>.</span><span class=n>begin</span><span class=p>(),</span>               <span class=c1>// output range
</span></span></span><span class=line><span class=cl><span class=c1></span>               <span class=n>ftor</span>                       <span class=c1>// transform fun
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><p>The difference versus the traditional invocation of <code>transform</code> is the
first parameter for setting up parallel policy hints. This parameter
tells the algorithm to use parallelization and vectorization in the
given case.</p><p>The official term for this param is <em><strong>Execution Policy</strong></em> <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. It
suggests the type of execution for the algorithm. As of current ISO C++
(C++ 23), there are four parallel policies:</p><ul><li><code>seq</code><br>An instance of <code>std::sequenced_policy</code>. It uses the sequential version
of the algorithm. The “classic” version of STL algorithms
(no Execution Policy param) is also supposed to be the same as this
one.</li><li><code>par</code><br>An instance of <code>std::parallel_policy</code>. The algorithm is allowed to
parallelize but disallowed to vectorize.</li><li><code>par_unseq</code><br>Instance of <code>std::parallel_unsequenced_policy</code>. The algorithm can be
both parallelized and vectorized.</li><li><code>unseq</code><br>Instance of <code>std::unsequenced_policy</code>. The algorithm is allowed to
vectorize but disallowed to parallelize.</li></ul><p>Therefore, the parallelizing effort is minimal. One could specify the
parallel policy to transform an existing STL algorithm into a parallel
(or vectorized) version.</p><p>However, STL algorithm implementations only considered the Execution
Policy as a hint or the maximum parallelization/vectorization level
allowed. They can ignore it and fall back to the serial execution
entirely.</p><p>Currently, most STL algorithms can take the Execution Policy parameter
as an instruction to run in parallel. Several new algorithms were added
to overcome the constraints of existing algorithms that forbid
parallelizing them or that there are better ways to express some
parallel algorithms (<code>reduce</code>, <code>exclusive_scan</code>, <code>inclusive_scan</code>,
<code>transform_reduce</code>, <code>transform_exclusive_scan</code>,
<code>transform_inclusive_scane</code>).</p><p>This article primarily focuses on parallel execution (<code>par</code> policy),
which aims to utilize all the available cores to increase efficiency.
Nonetheless, the author briefly touches on vectorization (<code>unseq</code>
policy) towards the end of the article <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><h1 id=3-difficulty-in-current-c-parallel-algorithms-design>3 Difficulty in current C++ Parallel Algorithms design<a hidden class=anchor aria-hidden=true href=#3-difficulty-in-current-c-parallel-algorithms-design>#</a></h1><h2 id=31-no-concurrency-awareness>3.1 No Concurrency awareness<a hidden class=anchor aria-hidden=true href=#31-no-concurrency-awareness>#</a></h2><p>The first thing to notice is that it is straightforward to adapt
existing algorithms and make them parallel. This improvement explains
the success of parallel algorithms (at least at the perception level).</p><p>Despite the C++ committee not intending to solve the concurrency problem
with STL Parallel (they wanted to solve local efficiency problems), it
also has a negative effect from a didactical point of view <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. C++
parallel algorithms have not allowed a global concurrency design. It
allows only local optimizations by making some algorithm calls
parallelizable. It is reasonable, for limited domains, to focus more on
efficiency than design, but that is typically different with
concurrency. One must pay attention to the design to get suboptimal
efficiency. In other words, multi-core efficiency is a global
optimization problem, not a local one. Considering this, the C++ users
might misunderstand that they do not need to pay attention to
concurrency issues; One can count on C++ STL to magically resolve them.</p><p>This problem is concluded to be the same problem that initially led
people to insufficient concurrency design. Instead of recognizing that
concurrency needs an entirely new type of design, the C++ standard tried
to apply sequential thinking by adding the ability to run on multiple
threads <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. Dropping the old ways of writing software and adopting a
new paradigm is necessary for proper concurrency.</p><h2 id=32-not-yet-completely-production-ready>3.2 Not yet completely production-ready<a hidden class=anchor aria-hidden=true href=#32-not-yet-completely-production-ready>#</a></h2><blockquote><p>the overall performance improvement gained by optimizing a single part
of a system is limited by the fraction of time that the improved part
is actually used</p><p>– Martin Reddy <a href=#ref-Martin2011>[11]</a> –</p></blockquote><p>Based on Amdahl’s law <a href=#ref-Amdahl1967>[12]</a>, <a href=#ref-David1985>[13]</a>, one needs to
have a significant part of the parallelizable code to have speed
improvements from parallelism. In the case of Parallel STL, one needs to
have considerable parts of the application using such algorithms. In
other words, the vast majority of the time must be spent on STL
algorithms to have relevant performance benefits.</p><p>However, not every program consists just of STL algorithm calls.
Currently, many programs have flows that are tough to reduce to STL
algorithms (if it is even possible). These applications can only expose
control flow concurrency, making them unsuitable for using parallel
algorithms. Imagine a context where the application is doing graph
processing. In most cases, no STL algorithms are ready yet to be used.</p><h2 id=33-combining-stl-algorithms-introduces-sequential-behavior>3.3 Combining STL algorithms introduces sequential behavior<a hidden class=anchor aria-hidden=true href=#33-combining-stl-algorithms-introduces-sequential-behavior>#</a></h2><p>Suppose a context where one needs to call multiple STL algorithms. If
the algorithms were to interact, then they need to be called serially:</p><div class=figure style=text-align:center><img src=images/stl-algorithm-flow.png alt="STL Algorithms Flow Illustration." width=100%><p class=caption>STL Algorithms Flow Illustration.</p></div><p>In the above figure, three flow parts are bound to execute sequentially:
before the first algorithm, between the algorithm calls, and at the end
of the second algorithm. Moreover, no matter how exemplary the
implementation is, the synchronizing and starting tasks need additional
time. Thus, according to Amdahl’s law <a href=#ref-Amdahl1967>[12]</a>,
<a href=#ref-David1985>[13]</a>, this puts an upper limit on performance improvement.</p><p>Nevertheless, whenever the algorithm ends, it must wait for all the
threads to finish executing, which reduces the machine’s
parallelism capacity. This behavior results from the fact that the work
cannot perfectly distribute between threads; several threads process
more than others.</p><p>To counter this problem, one could do one or more of the following:</p><ul><li>Start algorithms from multiple threads</li><li>Ensure that algorithms have continuations or pipelining, avoiding
serially calling algorithms</li><li>Tune the parallel algorithms to ensure the work’s dividing.</li></ul><p>The first item can be possible outside parallel STL without direct
support. However, the current design of the C++ standard library does
not allow one to implement any of the latter.</p><h2 id=34-small-datasets-are-not-suitable-for-parallelization>3.4 Small datasets are not suitable for parallelization<a hidden class=anchor aria-hidden=true href=#34-small-datasets-are-not-suitable-for-parallelization>#</a></h2><p>To make it worthwhile to parallelize an algorithm, the execution time of
the algorithm needs to be significant <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>. Teodorescu <a href=#ref-Teodorescu20a>[14]</a> has
refocused Amdahl’s Law <a href=#ref-Amdahl1967>[12]</a>, <a href=#ref-David1985>[13]</a> and
suggested that the algorithm needs to take more than 100 milliseconds to
be beneficial from parallelizing. Other optimization opportunities could
be better helpful if this is not the case.</p><p>Such algorithms (which have long execution times) either have a
sufficiently large number of elements (e.g., millions of elements) in
the collection, or the operation given to the algorithm needs to take a
long time. While the latter can be prevalent in many applications, the
former (considerable elements) is typically unpopular.</p><p>To give an order of magnitude for <em>numerous elements</em>, Bartlomiej
<a href=#ref-Filipek17b>[5]</a> benchmarked two experiments on a decent high-performance
machine. The first benchmark is a simple transform operation (the
functor doubling the value received). For 100k elements, the execution
times decrease from 0.524 to 0.389 seconds. A 1.3x improvement for six
cores machine is not worth parallelizing. After this test, Bartlomiej
writes <a href=#ref-Filipek17b>[5]</a>:</p><blockquote><p>As you see on the faster machine, you need like 1 million elements to
start seeing some performance gains. On the other hand, on my
notebook, all parallel implementations were slower.</p></blockquote><p>The last experiment is a real-world problem: Computing the Fresnel
transformation on 3D vectors. The parallelization improvement result is
sufficient, an approximate 6x improvement (from 1.697 seconds to 0.283
seconds) for 100k elements. Although these tests are biased, they still
present a rough observation: <strong>One needs a significant number of
elements (100k) for the performance improvement to be hundreds of
milliseconds</strong>.</p><p>Consider other benchmarks from the parallel algorithms implementor of
Microsoft Visual Studio’s standard library, Billy <a href=#ref-ONeal18>[6]</a>.
His article illustrates several benchmarks for sorting algorithms. The
experiment contexts use 1 million elements, resulting in less than 100
milliseconds.</p><p>To conclude, to see significant performance improvements from
parallelizing STL algorithms, containers with a rough order of magnitude
of 1 million elements must be made. This condition only happens
sometimes.</p><h2 id=35-stl-algorithms-is-not-tunning-friendly>3.5 STL Algorithms is not tunning-friendly<a hidden class=anchor aria-hidden=true href=#35-stl-algorithms-is-not-tunning-friendly>#</a></h2><p>This problem affects all the STL algorithms. Thus, the author takes the
<code>sort</code> algorithm as a running example. For a few elements, sorting is
faster with a linear algorithm than with a parallel one.
Production-ready algorithms typically have a threshold: if the number of
elements is below this point, the algorithm calls the serial version.
The cutoff point varies on algorithm implementations. Intel
OneTBB’s <code>parallel_sort</code> and GNU <code>libstdc++</code> set this value to
<code>500</code> <a href=#ref-IntelOneTBB>[15]</a>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>//! Sorts the data in [begin,end) using the given comparator
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cm>/** The compare function object is used for all comparisons between elements during sorting.
</span></span></span><span class=line><span class=cl><span class=cm>    The compare object must define a bool operator() function.
</span></span></span><span class=line><span class=cl><span class=cm>    @ingroup algorithms **/</span>
</span></span><span class=line><span class=cl><span class=k>template</span><span class=o>&lt;</span><span class=k>typename</span> <span class=n>RandomAccessIterator</span><span class=p>,</span> <span class=k>typename</span> <span class=n>Compare</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl>    <span class=n>__TBB_requires</span><span class=p>(</span><span class=n>std</span><span class=o>::</span><span class=n>random_access_iterator</span><span class=o>&lt;</span><span class=n>RandomAccessIterator</span><span class=o>&gt;</span> <span class=o>&amp;&amp;</span>
</span></span><span class=line><span class=cl>                   <span class=n>compare</span><span class=o>&lt;</span><span class=n>Compare</span><span class=p>,</span> <span class=n>RandomAccessIterator</span><span class=o>&gt;</span> <span class=o>&amp;&amp;</span>
</span></span><span class=line><span class=cl>                   <span class=n>std</span><span class=o>::</span><span class=n>movable</span><span class=o>&lt;</span><span class=n>iter_value_type</span><span class=o>&lt;</span><span class=n>RandomAccessIterator</span><span class=o>&gt;&gt;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=n>parallel_sort</span><span class=p>(</span> <span class=n>RandomAccessIterator</span> <span class=n>begin</span><span class=p>,</span> <span class=n>RandomAccessIterator</span> <span class=n>end</span><span class=p>,</span> <span class=k>const</span> <span class=n>Compare</span><span class=o>&amp;</span> <span class=n>comp</span> <span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>constexpr</span> <span class=kt>int</span> <span class=n>min_parallel_size</span> <span class=o>=</span> <span class=mi>500</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span><span class=p>(</span> <span class=n>end</span> <span class=o>&gt;</span> <span class=n>begin</span> <span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span><span class=p>(</span> <span class=n>end</span> <span class=o>-</span> <span class=n>begin</span> <span class=o>&lt;</span> <span class=n>min_parallel_size</span> <span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>std</span><span class=o>::</span><span class=n>sort</span><span class=p>(</span><span class=n>begin</span><span class=p>,</span> <span class=n>end</span><span class=p>,</span> <span class=n>comp</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>parallel_quick_sort</span><span class=p>(</span><span class=n>begin</span><span class=p>,</span> <span class=n>end</span><span class=p>,</span> <span class=n>comp</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>The value <code>500</code> is well-analyzed with primitive type experiments
(string, integers, floating-points) by STL vendors before being set as
the threshold. However, one size does not fit all; this cutoff point
must be different to sort some complex objects. Unfortunately, <strong>the C++
standard only aims to support the general purpose and does not allow any
customizing of the algorithms</strong>. This leads to suboptimal performance.</p><p>Similarly, given a simple <code>for_each</code> operation, if the algorithm needs
to call a heavy function, it is reasonable to have each element mapped
into a task. However, if the transformation is simple (i.e., an
arithmetic operation), creating one task per element is not ideal for
performance. Algorithms may need tuning to adjust different input
patterns.</p><p>Similarly, let’s assume a simple <code>for_each</code> operation. For some
cases, if the algorithm needs to call a heavy function, it’s ok to
have each element mapped into a task. On the other hand, if the
transformation is simple (i.e., an arithmetic operation) then, creating
one task per element will be bad for performance. Thus, algorithms may
need tuning to accommodate different input patterns.</p><p>Concore library <a href=#ref-concore>[16]</a> showing how one can set hints to parallel
algorithms:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>concore</span><span class=o>::</span><span class=n>partition_hints</span> <span class=n>hints</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>hints</span><span class=p>.</span><span class=n>granularity_</span> <span class=o>=</span> <span class=mi>100</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// cannot process less than 100 elements in
</span></span></span><span class=line><span class=cl><span class=c1>// a single task
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>concore</span><span class=o>::</span><span class=n>conc_for</span><span class=p>(</span><span class=n>input</span><span class=p>.</span><span class=n>begin</span><span class=p>(),</span> <span class=n>input</span><span class=p>.</span><span class=n>end</span><span class=p>(),</span> <span class=n>functor</span><span class=p>,</span> <span class=n>hints</span><span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=36-compile-time-overhead>3.6 Compile time overhead<a hidden class=anchor aria-hidden=true href=#36-compile-time-overhead>#</a></h2><p>C++ STL is heavily templatized and needs to be implemented in header
files. Even so, implementing parallel algorithms is far more complicated
than serial versions and adds significant compile-time overhead <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>.</p><p>People could not take this problem lightly since it takes more time to
compile C++ programs. Especially when applying Test-Driven-Development
in the project, compilation cycles take forever to compile.</p><p>Fortunately, there are a few tricks that both implementers can employ to
save part of this problem. Jorg introduced several techniques to reduce
compile time costs in CppCon 2019 <a href=#ref-Jorg19>[18]</a>. For example, GCC will
not compile the parallel versions of the library if <code>&lt;execution></code> is not
included <a href=#ref-Rodgers18>[17]</a>.</p><h1 id=4-conclusion>4 Conclusion<a hidden class=anchor aria-hidden=true href=#4-conclusion>#</a></h1><p>C++ parallel STL brings parallelization and vectorization to standard
algorithms without requiring much effort from the user. However, the
author considered this feature under-expected and needs better support.</p><p>Firstly, moving to multi-core programming requires a global concurrency
design. C++ parallel algorithms do not offer any support for this.
Moreover, the standard (accidentally) misguide to C++ users that
parallelism can be easily achieved without concurrent understanding.</p><p>Secondly, besides the high-level concurrency design issue, the standard
parallel library also has limitations at the low level. As applications
have more than just algorithms, using standard C++ parallel algorithms
is insufficient to achieve good speedups for most applications. The
article covers several problems related to lower-level performance
issues:</p><ul><li>STL algorithms are serialized.</li><li>Typically, Small datasets are unsuitable for parallelization</li><li>Impossibility of tuning the algorithms.</li><li>Compile time overhead.</li></ul><p>It is true that it’s easy for a user to quickly change the policy
of the STL algorithms and maybe get some performance benefits. But my
focus in this article is on the empty half of the glass. I’m
arguing that the benefits are not as big as one could obtain with a
proper concurrency design. In some limited cases (i.e., many elements,
or functors that are too complex) one might get some speedups for
one’s algorithms. But even in these cases, the costs of spiking
into using multiple cores may have an overall negative performance
costs.</p><p>All these make C++ parallel algorithms less of a great addition to the
concurrency frameworks. The author agrees that they are needed, but it
is still insufficient. The basic executor concept would have been a
better complement to the standard. Unfortunately, the executors did not
make it to C++ 23. So, let people wait to see what the future will
reserve.</p><h1 id=5-appendix>5 Appendix<a hidden class=anchor aria-hidden=true href=#5-appendix>#</a></h1><h1 id=6-references>6 References<a hidden class=anchor aria-hidden=true href=#6-references>#</a></h1><div id=refs class="references csl-bib-body"><div id=ref-Sutter05 class=csl-entry><p><span class=csl-left-margin>[1] </span><span class=csl-right-inline>H. Sutter, “The free lunch is over a
fundamental turn toward concurrency in software,” 2005.</span></p></div><div id=ref-iso-cpp-20 class=csl-entry><p><span class=csl-left-margin>[2] </span><span class=csl-right-inline>“<span class=nocase>Programming
languages - C++</span>,” International Organization for
Standardization; ISO/IEC 14882:2020, International Standard, Dec.
2020.Available: <a href=https://www.iso.org/standard/79358.html>https://www.iso.org/standard/79358.html</a></span></p></div><div id=ref-Lelbach16 class=csl-entry><p><span class=csl-left-margin>[3] </span><span class=csl-right-inline>B. A. Lelbach, “<span class=nocase>C++
Parallel Algorithms and Beyond</span>.” CppCon 2016 conference,
2016.Available: <a href="https://www.youtube.com/watch?v=UJrmee7o68A">https://www.youtube.com/watch?v=UJrmee7o68A</a></span></p></div><div id=ref-Filipek17a class=csl-entry><p><span class=csl-left-margin>[4] </span><span class=csl-right-inline>B. Filipek, “<span class=nocase>C++17
in details: Parallel Algorithms</span>.” C++ Stories, blog, 2017
[Online].Available:
<a href=https://www.cppstories.com/2017/08/cpp17-details-parallel/>https://www.cppstories.com/2017/08/cpp17-details-parallel/</a></span></p></div><div id=ref-Filipek17b class=csl-entry><p><span class=csl-left-margin>[5] </span><span class=csl-right-inline>B. Filipek, “<span class=nocase>The
Amazing Performance of C++17 Parallel Algorithms, is it
Possible?</span>” C++ Stories, blog, 2018 [Online].Available:
<a href=https://www.cppstories.com/2018/11/parallel-alg-perf/>https://www.cppstories.com/2018/11/parallel-alg-perf/</a></span></p></div><div id=ref-ONeal18 class=csl-entry><p><span class=csl-left-margin>[6] </span><span class=csl-right-inline>B. O’Neal, “<span class=nocase>Using C++17 Parallel Algorithms for Better
Performance</span>.” C++ Team Blog, blog, 2018
[Online].Available:
<a href=https://devblogs.microsoft.com/cppblog/using-c17-parallel-algorithms-for-better-performance/>https://devblogs.microsoft.com/cppblog/using-c17-parallel-algorithms-for-better-performance/</a></span></p></div><div id=ref-Parent16 class=csl-entry><p><span class=csl-left-margin>[7] </span><span class=csl-right-inline>S. Parent, “Better Code:
Concurrency.” code::dive 2016 conference, 2016.Available:
<a href="https://www.youtube.com/watch?v=QIHy8pXbneI">https://www.youtube.com/watch?v=QIHy8pXbneI</a></span></p></div><div id=ref-Pike13 class=csl-entry><p><span class=csl-left-margin>[8] </span><span class=csl-right-inline>R. Pike, “Concurrency Is Not
Parallelism.” Heroku’s Waza conference, 2012.Available:
<a href=https://go.dev/talks/2012/waza.slide#2>https://go.dev/talks/2012/waza.slide#2</a></span></p></div><div id=ref-Teodorescu20c class=csl-entry><p><span class=csl-left-margin>[9] </span><span class=csl-right-inline>L. R. Teodorescu, “Concurrency Design
Patterns,” <em>Overload</em>, vol. 159, pp. 12–18, 2020
[Online],Available:
<a href="https://accu.org/journals/overload/28/159/overload159.pdf#page=14">https://accu.org/journals/overload/28/159/overload159.pdf#page=14</a></span></p></div><div id=ref-Henney17 class=csl-entry><p><span class=csl-left-margin>[10] </span><span class=csl-right-inline>K. Henney, “<span class=nocase>Thinking
Outside the Synchronisation Quadrant</span>.” ACCU 2017
conference, 2017.Available:
<a href="https://www.youtube.com/watch?v=UJrmee7o68A">https://www.youtube.com/watch?v=UJrmee7o68A</a></span></p></div><div id=ref-Martin2011 class=csl-entry><p><span class=csl-left-margin>[11] </span><span class=csl-right-inline>M. Reddy, “Chapter 7 -
Performance,” in <em><span class=nocase>API Design for C++</span></em>,
M. Reddy, Ed. Boston: Morgan Kaufmann, 2011, pp. 209–240. doi:
<a href=https://doi.org/10.1016/B978-0-12-385003-4.00007-5>https://doi.org/10.1016/B978-0-12-385003-4.00007-5</a>.</span></p></div><div id=ref-Amdahl1967 class=csl-entry><p><span class=csl-left-margin>[12] </span><span class=csl-right-inline>G. M. Amdahl, “Validity of the single
processor approach to achieving large scale computing
capabilities,” in <em>Proceedings of the april 18-20, 1967, spring
joint computer conference</em>, 1967, pp. 483–485. doi:
<a href=https://doi.org/10.1145/1465482.1465560>10.1145/1465482.1465560</a>.</span></p></div><div id=ref-David1985 class=csl-entry><p><span class=csl-left-margin>[13] </span><span class=csl-right-inline>D. P. Rodgers, “Improvements in
multiprocessor system design,” <em>SIGARCH Comput. Archit. News</em>,
vol. 13, no. 3, pp. 225–231, Jun. 1985, doi:
<a href=https://doi.org/10.1145/327070.327215>10.1145/327070.327215</a>.</span></p></div><div id=ref-Teodorescu20a class=csl-entry><p><span class=csl-left-margin>[14] </span><span class=csl-right-inline>L. R. Teodorescu, “<span class=nocase>Refocusing Amdahl’s Law</span>,” <em>Overload</em>,
vol. 157, pp. 5–10, 2020 [Online],Available:
<a href="https://accu.org/journals/overload/28/157/overload157.pdf#page=7">https://accu.org/journals/overload/28/157/overload157.pdf#page=7</a></span></p></div><div id=ref-IntelOneTBB class=csl-entry><p><span class=csl-left-margin>[15] </span><span class=csl-right-inline>“<span class=nocase>oneAPI Threading
Building Blocks (oneTBB)</span>,” <em>GitHub repository</em>. Intel
Corporation;
<a href=https://github.com/oneapi-src/oneTBB/blob/master/include/oneapi/tbb/parallel_sort.h;>https://github.com/oneapi-src/oneTBB/blob/master/include/oneapi/tbb/parallel_sort.h;</a>
GitHub, 2023 [Online].</span></p></div><div id=ref-concore class=csl-entry><p><span class=csl-left-margin>[16] </span><span class=csl-right-inline>L. R. Teodorescu, <em><span class=nocase>Concore
library</span></em>. ver. 0.5, 2021 [Online].Available:
<a href=https://github.com/lucteo/concore>https://github.com/lucteo/concore</a></span></p></div><div id=ref-Rodgers18 class=csl-entry><p><span class=csl-left-margin>[17] </span><span class=csl-right-inline>T. Rodgers, “<span class=nocase>Bringing C++ 17 Parallel Algorithms to a Standard Library
Near You</span>.” CppCon 2018 conference, 2018.Available:
<a href="https://www.youtube.com/watch?v=-KT8gaojHUU">https://www.youtube.com/watch?v=-KT8gaojHUU</a></span></p></div><div id=ref-Jorg19 class=csl-entry><p><span class=csl-left-margin>[18] </span><span class=csl-right-inline>J. Brown, “<span class=nocase>Reducing
Template Compilation Overhead, Using C++11, 14, 17, and
20</span>.” CppCon 2019 conference, 2019.Available:
<a href="https://www.youtube.com/watch?v=TyiiNVA1syk">https://www.youtube.com/watch?v=TyiiNVA1syk</a></span></p></div></div><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>The author intercharges <em><strong>C++ Parallel STL</strong></em> term with <em><strong>C++
parallel algorithms</strong></em>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>According to ISO C++ 20 <a href=#ref-iso-cpp-20>[2]</a>:</p><ul><li><code>std::sequenced_policy</code></li></ul><blockquote><p>The execution policy type used as a unique type to disambiguate
parallel algorithm overloading and require that a parallel
algorithm’s execution may not be parallelized. The
invocations of element access functions in parallel algorithms
invoked with this policy (usually specified as
<code>std::execution::seq</code>) are indeterminately sequenced in the
calling thread.</p></blockquote><ul><li><code>std::parallel_policy</code></li></ul><blockquote><p>The execution policy type used as a unique type to disambiguate
parallel algorithm overloading and indicate that a parallel
algorithm’s execution may be parallelized. The invocations
of element access functions in parallel algorithms invoked with
this policy (usually specified as <code>std::execution::par</code>) are
permitted to execute in either the invoking thread or in a thread
implicitly created by the library to support parallel algorithm
execution. Any such invocations executing in the same thread are
indeterminately sequenced with respect to each other.</p></blockquote><ul><li><code>std::parallel_unsequenced_policy</code></li></ul><blockquote><p>The execution policy type used as a unique type to disambiguate
parallel algorithm overloading and indicate that a parallel
algorithm’s execution may be parallelized, vectorized, or
migrated across threads (such as by a parent-stealing scheduler).
The invocations of element access functions in parallel algorithms
invoked with this policy are permitted to execute in an unordered
fashion in unspecified threads, and unsequenced with respect to
one another within each thread.</p></blockquote><ul><li><code>std::unsequenced_policy</code></li></ul><blockquote><p>The execution policy type used as a unique type to disambiguate
parallel algorithm overloading and indicate that a parallel
algorithm’s execution may be vectorized, e.g., executed on a
single thread using instructions that operate on multiple data
items.</p></blockquote>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:3><p><a href=#ref-Lelbach16>[3]</a>–<a href=#ref-ONeal18>[6]</a> provided comprehensive introduction
and explanation of C++ parallel algorithms.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>In order to efficiently use vectorization without global
parallelism design, one typically must focus on the local
computations. The local-focus approach is perfect for applying
vectorization at the STL algorithms level: It can unlock a more
significant portion of the computation power available on modern
hardware <a href=#ref-Parent16>[7]</a>.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>The author discusses parallelism, not concurrency. The distinction
is well-explained by Pike and Teodorescu <a href=#ref-Pike13>[8]</a>, <a href=#ref-Teodorescu20c>[9]</a>. In
short, Concurrency is a design concern, while parallelism is a
run-time efficiency concern.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>This article supports Henney’s well-discussion on the
synchronization quadrant <a href=#ref-Henney17>[10]</a>.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><blockquote><p>Please excuse my over-generalisation, but it doesn’t seem
too rewarding to try to parallelize a 100 millisecond algorithm.
Obtaining a linear speedup on a 6-core machine would save 83.3
milliseconds on the completion time, and will fully utilise all
the cores.</p><p>– Lucian Radu Teodorescu <a href=#ref-Teodorescu20a>[14]</a> –</p></blockquote>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></li><li id=fn:8><p>Rodgers provided some metrics about this problem in CppCon 2018
<a href=#ref-Rodgers18>[17]</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><hr><footer class=post-footer><ul class=post-tags><li><a href=https://philong6297.github.io/tags/cpp/>cpp</a></li></ul><nav class=paginav><a class=prev href=https://philong6297.github.io/posts/a_case_study_on_performance_engineering_optimizing_cpp_binary_search/><span class=title>« Prev</span><br><span>A Case Study on Performance Engineering: Optimizing C++ Binary Search</span></a>
<a class=next href=https://philong6297.github.io/posts/dynamic_memory_cost_and_optimization_strategies_allocation_speed/><span class=title>Next »</span><br><span>Dymanic Memory Cost and Optimization Strategies: Allocation Speed</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 Phi-Long Le</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>