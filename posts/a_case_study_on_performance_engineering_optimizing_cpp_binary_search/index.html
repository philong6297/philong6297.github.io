<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Case Study on Performance Engineering: Optimizing C++ Binary Search | LongLP</title><meta name=keywords content="cpp"><meta name=description content="1 Introduction In my opinion, the most captivating demonstrations of performance engineering are from the optimization of textbook algorithms—those that are commonly known and seemingly too simple to warrant any further enhancement. These optimizations, however, prove to be enlightening and offer valuable insights that can be applied across a wide range of contexts. Remarkably, such optimization opportunities are more abundant than one might anticipate.
In this article, we will concentrate our attention on a fundamental algorithm known as binary search."><meta name=author content="longlp"><link rel=canonical href=https://philong6297.github.io/posts/a_case_study_on_performance_engineering_optimizing_cpp_binary_search/><link crossorigin=anonymous href=/assets/css/stylesheet.2e47ffddd5e3f5568a7da09828f5950ef07511824dcadb34c30c775ea3549464.css integrity="sha256-Lkf/3dXj9VaKfaCYKPWVDvB1EYJNyts0wwx3XqNUlGQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.8f4558fe13d35bf64986a19e09ff0c1eb791cc6d8c81193b52bdc48d24f1f22b.js integrity="sha256-j0VY/hPTW/ZJhqGeCf8MHreRzG2MgRk7Ur3EjSTx8is=" onload=hljs.highlightAll()></script>
<link rel=icon href=https://philong6297.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://philong6297.github.io/images/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://philong6297.github.io/images/favicon-32x32.png><link rel=apple-touch-icon href=https://philong6297.github.io/images/apple-touch-icon.png><link rel=mask-icon href=https://philong6297.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="A Case Study on Performance Engineering: Optimizing C++ Binary Search"><meta property="og:description" content="1 Introduction In my opinion, the most captivating demonstrations of performance engineering are from the optimization of textbook algorithms—those that are commonly known and seemingly too simple to warrant any further enhancement. These optimizations, however, prove to be enlightening and offer valuable insights that can be applied across a wide range of contexts. Remarkably, such optimization opportunities are more abundant than one might anticipate.
In this article, we will concentrate our attention on a fundamental algorithm known as binary search."><meta property="og:type" content="article"><meta property="og:url" content="https://philong6297.github.io/posts/a_case_study_on_performance_engineering_optimizing_cpp_binary_search/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-12T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-12T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Case Study on Performance Engineering: Optimizing C++ Binary Search"><meta name=twitter:description content="1 Introduction In my opinion, the most captivating demonstrations of performance engineering are from the optimization of textbook algorithms—those that are commonly known and seemingly too simple to warrant any further enhancement. These optimizations, however, prove to be enlightening and offer valuable insights that can be applied across a wide range of contexts. Remarkably, such optimization opportunities are more abundant than one might anticipate.
In this article, we will concentrate our attention on a fundamental algorithm known as binary search."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://philong6297.github.io/posts/"},{"@type":"ListItem","position":3,"name":"A Case Study on Performance Engineering: Optimizing C++ Binary Search","item":"https://philong6297.github.io/posts/a_case_study_on_performance_engineering_optimizing_cpp_binary_search/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Case Study on Performance Engineering: Optimizing C++ Binary Search","name":"A Case Study on Performance Engineering: Optimizing C\u002b\u002b Binary Search","description":"1 Introduction In my opinion, the most captivating demonstrations of performance engineering are from the optimization of textbook algorithms—those that are commonly known and seemingly too simple to warrant any further enhancement. These optimizations, however, prove to be enlightening and offer valuable insights that can be applied across a wide range of contexts. Remarkably, such optimization opportunities are more abundant than one might anticipate.\nIn this article, we will concentrate our attention on a fundamental algorithm known as binary search.","keywords":["cpp"],"articleBody":"1 Introduction In my opinion, the most captivating demonstrations of performance engineering are from the optimization of textbook algorithms—those that are commonly known and seemingly too simple to warrant any further enhancement. These optimizations, however, prove to be enlightening and offer valuable insights that can be applied across a wide range of contexts. Remarkably, such optimization opportunities are more abundant than one might anticipate.\nIn this article, we will concentrate our attention on a fundamental algorithm known as binary search. We introduce two variants of this algorithm that exhibit remarkable performance improvements over std::lower_bound, with the extent of enhancement dependent on the size of the problem at hand. Astonishingly, these variants achieve speeds up to four times faster:\nThe first variant accomplishes this feat by eliminating conditional branches, streamlining the search process and unlocking substantial performance gains. The second variant goes a step further by optimizing the memory layout, thereby capitalizing on the full potential of the cache system. It should be noted, however, that this optimization disqualifies it from serving as a drop-in replacement for std::lower_bound, as it necessitates preprocessing and array permutation before queries can be efficiently answered.\nThe benchmark environment is a moderate personal computer:\nProcessor: Intel(R) Xeon(R) CPU E3-1505M v5 @ 2.80 GHz (8 CPUs) RAM: 16GB Architecture: x86-64 Compilers: GNU/GCC 13.1 and LLVM/Clang 16.0 Implementation example: https://godbolt.org/z/r8rfzsv8f 2 Summary on Binary Search Below is the conventional method, commonly found in introductory computer science textbooks, for searching the first element in a sorted array array that is not less than a given value looking_for:\n1 2 3 4 5 6 7 8 9 10 11 12 13 auto lower_bound(const std::vector\u003cint32_t\u003e\u0026 array, int32_t looking_for) -\u003e int32_t { size_t l = 0; size_t r = array.size() - 1; while (l \u003c r) { const auto m = (l + r) / 2; if (array[m] \u003e= looking_for) r = m; else l = m + 1; } return array[l]; } The algorithm follows a straightforward approach: it identifies the middle element of the current search range, compares it to the target value looking_for, and then narrows down the search range in half until it becomes invalid.\nA similar strategy is utilized by the std::lower_bound function, with a slight variation to accommodate containers that have non-random-access iterators. Instead of using the endpoints of the search interval, std::lower_bound employs the first element of the container and the size of the search interval to perform the search. This allows for greater flexibility and compatibility with different container types. LLMV/Clang and GNU/GCC use this approach as well:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 template \u003c class _AlgPolicy, class _Iter, class _Sent, class _Type, class _Proj, class _Comp\u003e _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX20 _Iter __lower_bound_impl( _Iter __first, _Sent __last, const _Type\u0026 __value, _Comp\u0026 __comp, _Proj\u0026 __proj) { auto __len = _IterOps\u003c_AlgPolicy\u003e::distance(__first, __last); while (__len != 0) { auto __l2 = std::__half_positive(__len); _Iter __m = __first; _IterOps\u003c_AlgPolicy\u003e::advance(__m, __l2); if (std::__invoke(__comp, std::__invoke(__proj, *__m), __value)) { __first = ++__m; __len -= __l2 + 1; } else { __len = __l2; } } return __first; } If your compiler is sufficiently optimized, it will generate machine code that is comparable in efficiency and exhibits similar average latency. As we observed from our benchmarking results, the expected latency tends to increase with the size of the array:\nGiven that most developers do not typically implement binary search manually, we will consider std::lower_bound from LLVM/Clang as our baseline for comparison.\n2.1 The Peformance Issue Before delving into the optimized implementations, it is important to understand why binary search can be slow in its standard form.\nBy analyzing the assembly code of std::lower_bound using a performance profiling tool like perf, we can observe that a significant amount of time is spent on a conditional jump instruction:\n1 2 3 4 5 6 7 8 9 10 │35: mov %rax,%rdx 0.52 │ sar %rdx 0.33 │ lea (%rsi,%rdx,4),%rcx 4.30 │ cmp (%rcx),%edi 65.39 │ ↓ jle b0 0.07 │ sub %rdx,%rax 9.32 │ lea 0x4(%rcx),%rsi 0.06 │ dec %rax 1.37 │ test %rax,%rax 1.11 │ ↑ jg 35 This pipeline stall hinders the progress of the search, and it is primarily caused by two factors:\nControl Hazard: The presence of an unpredictable branch (due to independent random queries and keys) forces the processor to pause for 10-15 cycles to flush and refill the pipeline for each branch misprediction. Data Hazard: The preceding comparison operation must wait for one of its operands to be fetched from memory, which can introduce a delay ranging from 0 to 300 cycles, depending on the location of the operand. Now, let’s explore how we can overcome these obstacles one by one to improve the performance of binary search.\n3 Technical Contribution 3.1 Removing Branches We can introduce an optimized version of binary search that replaces branching with predication. To simplify the implementation, we adopt the approach used in the STL and modify the loop to use the first element and the size of the search interval:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 auto lower_bound(const std::vector\u003cint32_t\u003e\u0026 array, int32_t looking_for) -\u003e int32_t { size_t current_index = 0U; for (size_t length = array.size(); length \u003e 1;) { const auto half = length / 2; if (array[current_index + half - 1] \u003c looking_for) { current_index += half; length -= half; } else { length = half; } } return array[current_index]; } It is worth noting that in each iteration of the binary search algorithm, the variable length is effectively halved and then either floored or ceiled, depending on the outcome of the comparison. This conditional update appears to be unnecessary. To simplify the code, we can assume that length is always rounded up (ceiled) instead:\n1 2 3 4 5 6 7 8 9 10 11 auto lower_bound(const std::vector\u003cint32_t\u003e\u0026 array, int32_t looking_for) -\u003e int32_t { size_t current_index = 0U; for (size_t length = array.size(); length \u003e 1;) { const auto half = length / 2; if (array[current_index + half - 1] \u003c looking_for) current_index += half; length -= half; // = ceil(length / 2) } return array[current_index]; } In this way, it opens improvement chances when we just need to use a conditional move to update the first element of the search interval and halve the size on each phase:\n1 2 3 4 5 6 7 8 9 10 11 auto lower_bound(const std::vector\u003cint32_t\u003e\u0026 array, int32_t looking_for) -\u003e int32_t { size_t current_index = 0U; for (size_t length = array.size(); length \u003e 1;) { const auto half = length / 2; // will be replaced with a \"cmov\" current_index += (array[current_index + half - 1] \u003c looking_for) * half; length -= half; } return array[current_index]; } It is important to note that the modified loop in this implementation is not always equivalent to the standard binary search. Due to the rounding up of the search interval size, it accesses slightly different elements and may perform one comparison more than necessary. However, this modification simplifies the computations in each iteration and ensures a constant number of iterations if the array size remains constant. As a result, it completely eliminates branch mispredictions.\nHowever, it is worth mentioning that this technique, typical for predication, is sensitive to compiler optimizations. Depending on the specific compiler and how the function is invoked, it is possible that a branch may still be present or that suboptimal code may be generated.\nIn the case of Clang 16, this optimization performs well and provides a significant improvement of 2.5-3x on small arrays:\nAn interesting observation is that the branchless implementation performs worse on large arrays, which might seem counterintuitive. Considering that the total delay is primarily determined by RAM latency and that both the branchless and branchy versions perform similar memory accesses, one would expect their performance to be comparable or even slightly better for the branchless version.\nHowever, the key factor behind the better performance of the branchy version lies in the CPU’s ability to speculate on one of the branches. With branching, the CPU can start fetching either the left or right key before confirming which one is the correct choice. This speculative execution effectively acts as implicit prefetching, reducing the impact of memory latency.\nIn contrast, the branchless implementation lacks this speculative behavior since the cmov instruction is treated like any other instruction, and the branch predictor does not attempt to predict its operands to anticipate future execution. To compensate for this limitation, we can introduce prefetching by explicitly requesting the left and right child keys:\n1 2 3 4 5 6 7 8 9 10 11 12 auto lower_bound(const std::vector\u003cint32_t\u003e\u0026 array, int32_t looking_for) -\u003e int32_t { size_t current_index = 0U; for (size_t length = array.size(); length \u003e 1;) { const auto half = length / 2; length -= half; __builtin_prefetch(\u0026array[current_index + half - 1]); __builtin_prefetch(\u0026array[current_index + half * 2 - 1]); current_index += (array[current_index + half - 1] \u003c looking_for) * half; } return array[current_index]; } And now, the performance becomes roughly the same:\nAlthough the explicit prefetching in the branchless version helps improve its performance on large arrays, the graph still shows a faster growth compared to the branchy version. This is because the branchy version takes advantage of speculative execution to prefetch not only the immediate left or right child key but also “grandchildren,” “great-grandchildren,” and so on. Each additional speculative read, however, becomes exponentially less likely to be correct, diminishing the usefulness of these prefetches.\nIn the branchless version, it is possible to fetch ahead by more than one layer, similar to the branchy version. However, the number of fetches required would also grow exponentially, which is not a practical solution. Therefore, an alternative approach is needed to optimize memory operations.\n3.2 3.2. Optimizing the Layout Consider the following illustration:\nThe memory access pattern during binary search exhibits specific characteristics that impact the likelihood of elements being cached and the quality of their data locality. Let’s examine these aspects in more detail:\nSpatial locality seems relatively good for the last 3 to 4 requests, as they are likely to be within the same cache line. However, for all the previous requests, there are significant memory jumps, which can lead to poor spatial locality. This means that accessing elements in those requests may result in cache misses, requiring data to be fetched from main memory. Temporal Locality seems to be good for the first dozen of requests. This is because there are not that many different comparison sequences of this length, so the algorithm will repeatedly compare against the same middle elements. These frequently accessed elements are more likely to be cached. To further understand the importance of temporal locality and its impact on cache performance, an experiment is conducted where the element to compare is randomly chosen from within the search interval, rather than using the middle one.\nBy introducing randomness, the temporal locality is disrupted, and each comparison may involve different elements. This can lead to cache misses and potentially degrade performance compared to the standard binary search algorithm, where the middle element is consistently used for comparison:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // gen_random(l, r) will generate random number in [l, r] // use 32-bit Mersenne Twister and uniform random distribution auto lower_bound(const std::vector\u003cint32_t\u003e\u0026 array, int32_t looking_for) -\u003e int32_t { size_t l = 0; size_t r = array.size() - 1; while (l \u003c r) { const size_t m = gen_random(l, r); if (array[m] \u003e= x) r = m; else l = m + 1; } return array[l]; } Theoretically [1], the randomized binary search is expected to do 30-40% more comparisons. However the benchmarking result shows ~6x on large arrays:\nThe degraded performance in the randomized selection experiment is not solely due to the slow gen_random() call. The drop in performance can be observed at the L2-L3 cache boundary, indicating that the memory latency outweighs the overhead of random number generation and modulo operations. In this case, the performance degradation occurs because the fetched elements are unlikely to be cached, rather than just a small suffix of them.\nAnother potential negative effect to consider is cache associativity. If the array size is a multiple of a large power of two, the indices of the frequently accessed elements may be divisible by some large powers of two, resulting in cache conflicts. This can lead to cache thrashing, where elements mapped to the same cache line repeatedly evict each other, negatively impacting performance. For example, it takes about ~400ns/query for searching over arrays of size $2^{20}$ while searching over arrays of size $(2^{20} + 123)$ takes ~300ns - a 20% difference.\nFibonacci search is one of the way to resolve this problem. However the scope of this article will not talk about it, we are focusing on temporal locality issue. By that, let’s ignore any cache side effect by assuming all array sizes are $\\lfloor 1.17^k \\rfloor$ for integer $k$.\nThe primary issue with the memory layout is that it does not effectively leverage temporal locality. Specifically, the hot elements (frequently accessed) and cold elements (infrequently accessed) are grouped together in the same cache lines. For instance, the element $\\lfloor n/2 \\rfloor$, which is requested on each query, is likely stored in the same cache line as $\\lfloor n/2 \\rfloor + 1$, which is rarely requested. This grouping of hot and cold elements hampers the efficient utilization of temporal locality, leading to potential cache misses and performance degradation.\nGiven the heatmap that visualize the frequency of comparisons for an array of 31 elements:\nTo optimize the memory layout and enhance cache efficiency, it is desirable to group hot elements (frequently accessed) together and cold elements (infrequently accessed) together.\nTo achieve this cache-friendly memory layout, I suggest renumbering the elements using a 500 years-old numeration - The Eytzinger Layout.\n3.2.1 Eytzinger Layout According to Wiki:\nMichaël Eytzinger was an Austrian nobleman, diplomat, historian, and publicist, who wrote and published several works, including a renowned volume that states the principles of a genealogical numbering system, called an Ahnentafel, that is still in use today.\nIn the Ahnentafel (German for “ancestor table”) system, a person’s direct ancestors are numbered in a fixed sequence of ascent. The person themselves is assigned the number 1, and their ancestors are numbered recursively. For each person with number $k$, their father is assigned the number $2k$, and their mother is assigned the number $(2k+1)$.\nIn computer science, this enumeration technique has found applications in various data structures, particularly in implicit (pointer-free) implementations of binary tree structures such as heaps and segment trees. Instead of storing names, these structures use the Ahnentafel numbering system to assign numbers to the underlying array items, enabling efficient traversal and manipulation of the tree structure.\nConsider how the layout looks in our example:\nWith this layout, we are easily to start from the first element, then jump to either $2 k$ or $(2k + 1)$:\nBy reordering the elements of the array according to the Eytzinger layout, we can exploit the inherent structure of the binary tree and achieve optimal temporal locality. Elements closer to the root of the tree, which correspond to smaller numbers in the ahnentafel numbering, are placed closer to the beginning of the array. As a result, they are more likely to be fetched from the cache, leading to improved performance.\nThe process of reordering the elements using the Eytzinger layout follows a specific pattern. Even-indexed elements are written to the end of the new array, while the remaining elements are written in a similar manner, preserving their relative order within each level of the binary tree. The first element will be placed as the root of the tree.\n3.2.1.1 Building the layout To construct the layout, we can follow an article from Codeforces, in $O(\\log n)$ times:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // array: original sorted array // eytzinger_array: 1-indexed array // array_index will start from 0 // eytzinger_index will start from 1 (named as |k| in formula) auto build_eytzinger_array( const std::vector\u003cint32_t\u003e\u0026 array, std::vector\u003cint32_t\u003e\u0026 eytzinger_array, size_t array_index, size_t eytzinger_index) -\u003e size_t { if (eytzinger_index \u003c= array.size()) { const auto eytzinger_index_x2 = eytzinger_index * 2; array_index = build_eytzinger_array( array, eytzinger_array, array_index, eytzinger_index_x2); eytzinger_array[eytzinger_index] = array[array_index]; ++array_index; array_index = build_eytzinger_array( array, eytzinger_array, array_index, eytzinger_index_x2 + 1); } return array_index; } This function takes the current node number eytzinger_index and recursively constructs the Eytzinger array. It writes out the elements to the left of the middle of the search interval, then adds the current element for comparison, and finally writes out the elements to the right. Although it may appear complex at first glance, a few observations can help understand its functionality:\nIt writes exactly array.size() elements, as the if block is entered once for each eytzinger_index ranging from 1 to array.size(). The elements from the original array are written sequentially, incrementing the array_index with each iteration. By the time the element at node eytzinger_index is written, all the elements to its left (up to array_index) have already been written. Despite its recursive nature, the function is quite efficient due to the sequential memory reads and writes ($O(\\log n)$ at a time). However, maintaining the permutation can be more challenging both conceptually and computationally. Unlike adding an element to a sorted array, which involves shifting a suffix of elements, the Eytzinger array needs to be rebuilt entirely.\nAlthough the new layout may not be exactly equivalent to the “tree” structure of a standard binary search (For instance, the left child subtree may have a larger size than the right child subtree, potentially up to twice as large), this discrepancy does not significantly impact the overall performance, as both approaches still result in a binary search tree with a depth of approximately $\\lceil \\log_2 n \\rceil$.\nAdditionally, it’s worth mentioning that the Eytzinger array is 1-indexed, which holds importance for optimizing performance later. You can assign a value to the 0-th element to indicate the desired return value when the lower bound does not exist, like using array.end() for std::lower_bound.\n3.2.1.2 Implement the Searching Now, with the Eytzinger array and its optimized traversal, we can descend the array using only indices, eliminating the need to store and recalculate search boundaries. This efficient approach also allows us to avoid branching in our algorithm. Here’s the code snippet:\n1 2 3 4 5 // named as |k| in formula size_t current_eytzinger_index = 1; while (current_eytzinger_index \u003c= array.size()) current_eytzinger_index = 2 * current_eytzinger_index + (eytzinger_array[k] \u003c looking_for); By starting with $k=1$ and updating it as $k = 2 * k + (eytzinger\\_array[k] \u003c looking\\_for)$, we can navigate through the array efficiently, moving left if the current element is greater than or equal to $looking_for$, and right otherwise. The simplicity and lack of branching in this loop contribute to improved performance.\nHowever, a challenge arises when we need to determine the index of the resulting element, as $k$ does not directly point to it. Let’s consider an example to understand this issue (corresponding tree provided above):\narray: 0 1 2 3 4 5 6 7 8 9 eytzinger: 6 3 7 1 5 8 9 0 2 4 1st range: ------------?------ k := 2*k = 2 (6 \u003e= 3) 2nd range: ------?------ k := 2*k = 4 (3 \u003e= 3) 3rd range: --?---- k := 2*k + 1 = 9 (1 \u003c 3) 4th range: ?-- k := 2*k + 1 = 19 (2 \u003c 3) 5th range: ! In this example, we query the array of $[0, …, 9]$ for the lower bound of $x=3$. We compare $x$ against $[6, 3, 1, 2]$, proceeding with left-left-right-right steps until we end up with $k=19$. However, $k=19$ is not a valid array index.\nBy observing the flow, we can recognize that we can compare $x$ against it at some point unless the answer is the end of the array. Once we determine that the answer is not less than $x$, we make a left turn and then continue going right until we reach a leaf (as we only compare $x$ against lesser elements). Therefore, to restore the correct answer, “cancel” some right turns and then make one additional move.\nA solution is to recognize that the right turns are recorded as 1-bits in the binary representation of $k$. Therefore, finding the number of trailing 1-bits then right-shifting $k$ by that number plus one is the way to restore the answer. This can be achieved by inverting the number (~k) and letting the __builtin_ffs (or its variants) come to play:\n1 2 3 4 5 6 7 8 9 10 11 12 auto eytzinger_search( const std::vector\u003cint32_t\u003e\u0026 eytzinger_array, int32_t looking_for) -\u003e int32_t { // named as |k| in formulas size_t current_index = 1; while (current_index \u003c eytzinger_array.size()) { current_index = (current_index * 2) + (eytzinger_array[current_index] \u003c looking_for); } current_index \u003e\u003e= __builtin_ffs(~current_index); return eytzinger_array[current_index]; } Unfortunately, the result is quite sad:\nWhen comparing the latency of smaller arrays, the branchless binary search implementation and the Eytzinger Search exhibit similar performance. This similarity is expected since the branchless implementation is concise and efficient. However, as the array size increases, the performance of the Eytzinger Search lose the run.\nThe reason behind this performance difference lies in the loss of spatial locality in the Eytzinger Search. In the branchless implementation, the last 3-4 elements being compared are likely to reside in the same cache line, allowing for efficient memory access. However, in the Eytzinger layout, these last elements are no longer grouped together in a single cache line, resulting in separate memory fetches for each element.\nOne might argue that the improved temporal locality in the Eytzinger Search should compensate for the loss of spatial locality. Previously, only a fraction (approximately $\\frac{1}{16}$) of the cache line was utilized to store a hot element, whereas now the entire cache line is used, effectively increasing the effective cache size by a factor of 16. This expanded cache size should enable covering approximately 4 more initial requests based on logarithmic calculations.\nHowever, upon closer examination, it becomes apparent that this compensation is not sufficient. Caching the other 15 elements in the cache line was not entirely useless, and the hardware could have fetched neighboring cache lines in advance. If the last requests in the sequence were performed, the subsequent memory reads would likely involve accessing cached elements. Therefore, in reality, it is more likely that the last 6-7 accesses would benefit from caching, rather than just 3-4.\nAt first glance, it may seem that adopting the Eytzinger layout was an imprudent decision. However, let’s spend our efforts to make it worthwhile.\n3.2.1.3 On trying to prefetch To mitigate the impact of memory latency in the Eytzinger Search, we can employ prefetching techniques similar to those used in the branchless binary search. However, in the case of this layout, we can optimize the prefetching process by leveraging the sequential nature of neighboring nodes in the array.\nIn the Eytzinger array, the left ($2k$) and right child nodes ($(2k + 1)$) of a given node, are likely to reside in the same cache line. This observation allows us to issue a single prefetch instruction for both child nodes, reducing the overhead associated with separate prefetches.\nLet extends this observation further to the grand-children of node $k$, which are stored sequentially in the array. By examining the indices, we can deduce that the cache line containing these grand-children can also be fetched using a single instruction. Specifically, the indices $4k$, $4k + 1$, $4k + 2$, and $4k + 3$ correspond to the great-grandchildren of node $k$.\nWe can continue this pattern and prefetch even more descendants in advance. By fetching ahead as many as possible within a single cache line, we can prefetch up to $\\frac{64}{4} = 16$ elements, which are the great-great-grandchildren of a node $k$, spanning indices from $16k$ to $(16k + 15)$.\nTo ensure that we retrieve all $16$ elements with a single memory request, we need to take cache line boundaries into consideration. By prefetching both the first and last element of this group, we increase the chance of capturing all of them. Crucially, we can exploit the fact that the index of the first element, $16k$, is divisible by $16$. Consequently, the memory address of this element will be the base address of the array plus a value divisible by $64$, which is also the cache line size.\nThus, if the array starts on a cache line boundary, we can guarantee that these $16$ elements will reside entirely within a single cache line.\nTherefore, why not align the array by a magic $64$?\n1 2 // 1-indexed alignas(64) std::vector\u003cint32_t\u003e eytzinger_array(array.size() + 1); And then prefetch the first in the group of $16k$ on every iteration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 auto eytzinger_search( const std::vector\u003cint32_t\u003e\u0026 eytzinger_array, int32_t looking_for) -\u003e int32_t { // named as |k| in formulas size_t current_index = 1; while (current_index \u003c eytzinger_array.size()) { __builtin_prefetch(eytzinger_array.data() + (current_index * 16)); current_index = (current_index * 2) + (eytzinger_array[current_index] \u003c looking_for); } current_index \u003e\u003e= __builtin_ffs(~current_index); return eytzinger_array[current_index]; } And here is the result:\nThe performance improvement achieved by the prefetching technique in large arrays is quite significant, approximately 3-4 times faster compared to the previous version and around 2 times faster than std::lower_bound. This enhancement effectively hides memory latency by prefetching data four steps ahead and overlapping memory requests.\nIn theory, if computation was not a limiting factor, we would expect a speedup of approximately 4 times. However, in practice, the speedup is somewhat more moderate due to various factors.\nFurthermore, it’s worth considering prefetching beyond the 4 steps mentioned earlier. We can attempt to request only the first cache line and rely on the hardware to prefetch its neighboring cache lines. It’s important to note that the effectiveness of this trick depends on the specific hardware in use, and it may or may not improve overall performance.\nAdditionally, one should bear in mind that the prefetch requests may not be necessary (typically the last prefetch) and could potentially be invalid (out of the program allocated memory). Although modern CPUs treat these invalid instructions as no-ops (which means there are no slow-down problems), one should unroll the last few iterations out of the loop to eliminate unnecessary prefetches.\nFinally, prefetching does come at a cost. It effectively trades excess memory bandwidth for reduced latency. Therefore, for instance, if you run multiple instances simultaneously on separate hardware threads, it can significantly impact the benchmark performance.\n4 References [1] G. G. Brown and B. O. Shubert, “On random binary trees,” Mathematics of Operations Research, vol. 9, no. 1, pp. 43–65, 1984, Accessed: Jun. 13, 2023. [Online]. Available: http://www.jstor.org/stable/3689501\n[2] P.-V. Khuong and P. Morin, “Array layouts for comparison-based searching,” ACM J. Exp. Algorithmics, vol. 22, May 2017, doi: 10.1145/3053370.\n[3] M. Skarupke, “Beautiful branchless binary search.” PROBABLY DANCE, 2023.Available: https://probablydance.com/2023/04/27/beautiful-branchless-binary-search/\n","wordCount":"4522","inLanguage":"en","datePublished":"2023-06-12T00:00:00Z","dateModified":"2023-06-12T00:00:00Z","author":{"@type":"Person","name":"longlp"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://philong6297.github.io/posts/a_case_study_on_performance_engineering_optimizing_cpp_binary_search/"},"publisher":{"@type":"Organization","name":"LongLP","logo":{"@type":"ImageObject","url":"https://philong6297.github.io/images/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://philong6297.github.io accesskey=h title="LongLP (Alt + H)">LongLP</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://philong6297.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://philong6297.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://philong6297.github.io>Home</a>&nbsp;»&nbsp;<a href=https://philong6297.github.io/posts/>Posts</a></div><h1 class=post-title>A Case Study on Performance Engineering: Optimizing C++ Binary Search</h1><div class=post-meta><span title='2023-06-12 00:00:00 +0000 UTC'>June 12, 2023</span>&nbsp;·&nbsp;22 min&nbsp;·&nbsp;longlp</div><link id=katex-style rel=stylesheet href=/css/katex/katex.min.min.4725d79bb35c34627f6a1af14eaec611070ea67d628f36b032aaab7aceb35e20.css integrity="sha256-RyXXm7NcNGJ/ahrxTq7GEQcOpn1ijzawMqqres6zXiA=" crossorigin=anonymous media=screen><script id=katex-scripts src=/js/katex.min.min.4c32ddc568c1e882bbb8a987f40c3e66f83ba23e03a385c6bdebe700b8d11549.js integrity="sha256-TDLdxWjB6IK7uKmH9Aw+Zvg7oj4Do4XGvevnALjRFUk="></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script src=/js/reorder_refs.min.fd945cc9618b50ec332e28d44d448c34f1c06746f690df382a29d4390f2f2818.js integrity="sha256-/ZRcyWGLUOwzLijUTUSMNPHAZ0b2kN84KinUOQ8vKBg="></script>
<link rel=stylesheet href=/css/custom.min.1778a9cebde69f3ae78153408588708a16bfcf81e7a7c2579eba3d38a3f3f7e5.css integrity="sha256-F3ipzr3mnzrngVNAhYhwiha/z4Hnp8JXnro9OKPz9+U=" crossorigin=anonymous media=screen></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-introduction>1 Introduction</a></li><li><a href=#2-summary-on-binary-search>2 Summary on Binary Search</a><ul><li><a href=#21-the-peformance-issue>2.1 The Peformance Issue</a></li></ul></li><li><a href=#3-technical-contribution>3 Technical Contribution</a><ul><li><a href=#31-removing-branches>3.1 Removing Branches</a></li><li><a href=#32-32-optimizing-the-layout>3.2 3.2. Optimizing the Layout</a><ul><li><a href=#321-eytzinger-layout>3.2.1 Eytzinger Layout</a></li></ul></li></ul></li><li><a href=#4-references>4 References</a></li></ul></nav></div></details></div><div class=post-content><h1 id=1-introduction>1 Introduction<a hidden class=anchor aria-hidden=true href=#1-introduction>#</a></h1><p>In my opinion, the most captivating demonstrations of performance
engineering are from the optimization of textbook algorithms—those
that are commonly known and seemingly too simple to warrant any further
enhancement. These optimizations, however, prove to be enlightening and
offer valuable insights that can be applied across a wide range of
contexts. Remarkably, such optimization opportunities are more abundant
than one might anticipate.</p><p>In this article, we will concentrate our attention on a fundamental
algorithm known as <em><strong>binary search</strong></em>. We introduce two variants of
this algorithm that exhibit remarkable performance improvements over
<code>std::lower_bound</code>, with the extent of enhancement dependent on the size
of the problem at hand. Astonishingly, these variants achieve speeds up
to four times faster:</p><ul><li>The first variant accomplishes this feat by eliminating conditional
branches, streamlining the search process and unlocking substantial
performance gains.</li><li>The second variant goes a step further by optimizing the memory
layout, thereby capitalizing on the full potential of the cache
system.</li></ul><p>It should be noted, however, that this optimization disqualifies it from
serving as a drop-in replacement for <code>std::lower_bound</code>, as it
necessitates preprocessing and array permutation before queries can be
efficiently answered.</p><p>The benchmark environment is a moderate personal computer:</p><ul><li>Processor: Intel(R) Xeon(R) CPU E3-1505M v5 @ 2.80 GHz (8 CPUs)</li><li>RAM: 16GB</li><li>Architecture: x86-64</li><li>Compilers: GNU/GCC 13.1 and LLVM/Clang 16.0</li><li>Implementation example: <a href=https://godbolt.org/z/r8rfzsv8f>https://godbolt.org/z/r8rfzsv8f</a></li></ul><h1 id=2-summary-on-binary-search>2 Summary on Binary Search<a hidden class=anchor aria-hidden=true href=#2-summary-on-binary-search>#</a></h1><p>Below is the conventional method, commonly found in introductory
computer science textbooks, for searching the first element in a sorted
array <code>array</code> that is not less than a given value <code>looking_for</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>auto</span> <span class=nf>lower_bound</span><span class=p>(</span><span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>array</span><span class=p>,</span> <span class=kt>int32_t</span> <span class=n>looking_for</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=o>-&gt;</span> <span class=kt>int32_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>l</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>r</span> <span class=o>=</span> <span class=n>array</span><span class=p>.</span><span class=n>size</span><span class=p>()</span> <span class=o>-</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>while</span> <span class=p>(</span><span class=n>l</span> <span class=o>&lt;</span> <span class=n>r</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=k>auto</span> <span class=n>m</span> <span class=o>=</span> <span class=p>(</span><span class=n>l</span> <span class=o>+</span> <span class=n>r</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>array</span><span class=p>[</span><span class=n>m</span><span class=p>]</span> <span class=o>&gt;=</span> <span class=n>looking_for</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>r</span> <span class=o>=</span> <span class=n>m</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span>
</span></span><span class=line><span class=cl>      <span class=n>l</span> <span class=o>=</span> <span class=n>m</span> <span class=o>+</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>array</span><span class=p>[</span><span class=n>l</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>The algorithm follows a straightforward approach: it identifies the
middle element of the current search range, compares it to the target
value looking_for, and then narrows down the search range in half until
it becomes invalid.</p><p>A similar strategy is utilized by the <code>std::lower_bound</code> function, with
a slight variation to accommodate containers that have non-random-access
iterators. Instead of using the endpoints of the search interval,
<code>std::lower_bound</code> employs the first element of the container and the
size of the search interval to perform the search. This allows for
greater flexibility and compatibility with different container types.
<a href=https://github.com/llvm/llvm-project/blob/main/libcxx/include/__algorithm/lower_bound.h>LLMV/Clang</a> and GNU/GCC use this approach as well:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>_AlgPolicy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>_Iter</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>_Sent</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>_Type</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>_Proj</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=k>class</span> <span class=nc>_Comp</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>_LIBCPP_HIDE_FROM_ABI</span> <span class=n>_LIBCPP_CONSTEXPR_SINCE_CXX20</span> <span class=n>_Iter</span> <span class=n>__lower_bound_impl</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>_Iter</span> <span class=n>__first</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>_Sent</span> <span class=n>__last</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>_Type</span><span class=o>&amp;</span> <span class=n>__value</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>_Comp</span><span class=o>&amp;</span> <span class=n>__comp</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>_Proj</span><span class=o>&amp;</span> <span class=n>__proj</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>auto</span> <span class=n>__len</span> <span class=o>=</span> <span class=n>_IterOps</span><span class=o>&lt;</span><span class=n>_AlgPolicy</span><span class=o>&gt;::</span><span class=n>distance</span><span class=p>(</span><span class=n>__first</span><span class=p>,</span> <span class=n>__last</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>while</span> <span class=p>(</span><span class=n>__len</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>auto</span> <span class=n>__l2</span> <span class=o>=</span> <span class=n>std</span><span class=o>::</span><span class=n>__half_positive</span><span class=p>(</span><span class=n>__len</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>_Iter</span> <span class=n>__m</span> <span class=o>=</span> <span class=n>__first</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>_IterOps</span><span class=o>&lt;</span><span class=n>_AlgPolicy</span><span class=o>&gt;::</span><span class=n>advance</span><span class=p>(</span><span class=n>__m</span><span class=p>,</span> <span class=n>__l2</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>std</span><span class=o>::</span><span class=n>__invoke</span><span class=p>(</span><span class=n>__comp</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>__invoke</span><span class=p>(</span><span class=n>__proj</span><span class=p>,</span> <span class=o>*</span><span class=n>__m</span><span class=p>),</span> <span class=n>__value</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>__first</span> <span class=o>=</span> <span class=o>++</span><span class=n>__m</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=n>__len</span> <span class=o>-=</span> <span class=n>__l2</span> <span class=o>+</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>__len</span> <span class=o>=</span> <span class=n>__l2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>__first</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>If your compiler is sufficiently optimized, it will generate machine
code that is comparable in efficiency and exhibits similar average
latency. As we observed from our benchmarking results, the expected
latency tends to increase with the size of the array:</p><img src=images/search-std.webp width=80% style=display:block;margin:auto><p>Given that most developers do not typically implement binary search
manually, we will consider <code>std::lower_bound</code> from LLVM/Clang as our
baseline for comparison.</p><h2 id=21-the-peformance-issue>2.1 The Peformance Issue<a hidden class=anchor aria-hidden=true href=#21-the-peformance-issue>#</a></h2><p>Before delving into the optimized implementations, it is important to
understand why binary search can be slow in its standard form.</p><p>By analyzing the assembly code of <code>std::lower_bound</code> using a performance
profiling tool like <a href=https://perf.wiki.kernel.org/index.php/Main_Page>perf</a>, we can observe that a significant amount of
time is spent on a <strong>conditional jump</strong> instruction:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-asm data-lang=asm><span class=line><span class=cl>       <span class=err>│35:</span>   <span class=nf>mov</span>    <span class=nv>%rax</span><span class=p>,</span><span class=nv>%rdx</span>
</span></span><span class=line><span class=cl>  <span class=err>0</span><span class=nf>.52</span> <span class=err>│</span>      <span class=no>sar</span>    <span class=nv>%rdx</span>
</span></span><span class=line><span class=cl>  <span class=err>0</span><span class=nf>.33</span> <span class=err>│</span>      <span class=no>lea</span>    <span class=p>(</span><span class=nv>%rsi</span><span class=p>,</span><span class=nv>%rdx</span><span class=p>,</span><span class=mi>4</span><span class=p>),</span><span class=nv>%rcx</span>
</span></span><span class=line><span class=cl>  <span class=err>4</span><span class=nf>.30</span> <span class=err>│</span>      <span class=no>cmp</span>    <span class=p>(</span><span class=nv>%rcx</span><span class=p>),</span><span class=nv>%edi</span>
</span></span><span class=line><span class=cl> <span class=err>65</span><span class=nf>.39</span> <span class=err>│</span>    <span class=err>↓</span> <span class=no>jle</span>    <span class=no>b0</span>
</span></span><span class=line><span class=cl>  <span class=err>0</span><span class=nf>.07</span> <span class=err>│</span>      <span class=no>sub</span>    <span class=nv>%rdx</span><span class=p>,</span><span class=nv>%rax</span>
</span></span><span class=line><span class=cl>  <span class=err>9</span><span class=nf>.32</span> <span class=err>│</span>      <span class=no>lea</span>    <span class=mi>0x4</span><span class=p>(</span><span class=nv>%rcx</span><span class=p>),</span><span class=nv>%rsi</span>
</span></span><span class=line><span class=cl>  <span class=err>0</span><span class=nf>.06</span> <span class=err>│</span>      <span class=no>dec</span>    <span class=nv>%rax</span>
</span></span><span class=line><span class=cl>  <span class=err>1</span><span class=nf>.37</span> <span class=err>│</span>      <span class=no>test</span>   <span class=nv>%rax</span><span class=p>,</span><span class=nv>%rax</span>
</span></span><span class=line><span class=cl>  <span class=err>1</span><span class=nf>.11</span> <span class=err>│</span>    <span class=err>↑</span> <span class=no>jg</span>     <span class=mi>35</span>
</span></span></code></pre></td></tr></table></div></div><p>This <strong>pipeline stall</strong> hinders the progress of the search, and it is
primarily caused by two factors:</p><ul><li><strong>Control Hazard</strong>: The presence of an unpredictable branch (due to
independent random queries and keys) forces the processor to pause for
10-15 cycles to flush and refill the pipeline for each branch
misprediction.</li><li><strong>Data Hazard</strong>: The preceding comparison operation must wait for one
of its operands to be fetched from memory, which can introduce a delay
ranging from 0 to 300 cycles, depending on the location of the
operand.</li></ul><p>Now, let’s explore how we can overcome these obstacles one by one
to improve the performance of binary search.</p><h1 id=3-technical-contribution>3 Technical Contribution<a hidden class=anchor aria-hidden=true href=#3-technical-contribution>#</a></h1><h2 id=31-removing-branches>3.1 Removing Branches<a hidden class=anchor aria-hidden=true href=#31-removing-branches>#</a></h2><p>We can introduce an optimized version of binary search that replaces
branching with predication. To simplify the implementation, we adopt the
approach used in the STL and modify the loop to use the first element
and the size of the search interval:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>auto</span> <span class=nf>lower_bound</span><span class=p>(</span><span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>array</span><span class=p>,</span> <span class=kt>int32_t</span> <span class=n>looking_for</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=o>-&gt;</span> <span class=kt>int32_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>current_index</span> <span class=o>=</span> <span class=mi>0U</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=n>size_t</span> <span class=n>length</span> <span class=o>=</span> <span class=n>array</span><span class=p>.</span><span class=n>size</span><span class=p>();</span> <span class=n>length</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=k>auto</span> <span class=n>half</span> <span class=o>=</span> <span class=n>length</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>array</span><span class=p>[</span><span class=n>current_index</span> <span class=o>+</span> <span class=n>half</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>looking_for</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>current_index</span> <span class=o>+=</span> <span class=n>half</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=n>length</span> <span class=o>-=</span> <span class=n>half</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>length</span> <span class=o>=</span> <span class=n>half</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>array</span><span class=p>[</span><span class=n>current_index</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>It is worth noting that in each iteration of the binary search
algorithm, the variable <code>length</code> is effectively halved and then either
floored or ceiled, depending on the outcome of the comparison. This
conditional update appears to be unnecessary. To simplify the code, we
can assume that <code>length</code> is always rounded up (ceiled) instead:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>auto</span> <span class=nf>lower_bound</span><span class=p>(</span><span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>array</span><span class=p>,</span> <span class=kt>int32_t</span> <span class=n>looking_for</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=o>-&gt;</span> <span class=kt>int32_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>current_index</span> <span class=o>=</span> <span class=mi>0U</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=n>size_t</span> <span class=n>length</span> <span class=o>=</span> <span class=n>array</span><span class=p>.</span><span class=n>size</span><span class=p>();</span> <span class=n>length</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=k>auto</span> <span class=n>half</span> <span class=o>=</span> <span class=n>length</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>array</span><span class=p>[</span><span class=n>current_index</span> <span class=o>+</span> <span class=n>half</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>looking_for</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>current_index</span> <span class=o>+=</span> <span class=n>half</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>length</span> <span class=o>-=</span> <span class=n>half</span><span class=p>;</span> <span class=c1>// = ceil(length / 2)
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>array</span><span class=p>[</span><span class=n>current_index</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>In this way, it opens improvement chances when we just need to use a
<em><strong>conditional move</strong></em> to update the first element of the search
interval and halve the size on each phase:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>auto</span> <span class=nf>lower_bound</span><span class=p>(</span><span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>array</span><span class=p>,</span> <span class=kt>int32_t</span> <span class=n>looking_for</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=o>-&gt;</span> <span class=kt>int32_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>current_index</span> <span class=o>=</span> <span class=mi>0U</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=n>size_t</span> <span class=n>length</span> <span class=o>=</span> <span class=n>array</span><span class=p>.</span><span class=n>size</span><span class=p>();</span> <span class=n>length</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=k>auto</span> <span class=n>half</span> <span class=o>=</span> <span class=n>length</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// will be replaced with a &#34;cmov&#34;
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>current_index</span> <span class=o>+=</span> <span class=p>(</span><span class=n>array</span><span class=p>[</span><span class=n>current_index</span> <span class=o>+</span> <span class=n>half</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>looking_for</span><span class=p>)</span> <span class=o>*</span> <span class=n>half</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>length</span> <span class=o>-=</span> <span class=n>half</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>array</span><span class=p>[</span><span class=n>current_index</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>It is important to note that the modified loop in this implementation is
not always equivalent to the standard binary search. Due to the rounding
up of the search interval size, it accesses slightly different elements
and may perform one comparison more than necessary. However, this
modification simplifies the computations in each iteration and ensures a
constant number of iterations if the array size remains constant. As a
result, it completely eliminates branch mispredictions.</p><p>However, it is worth mentioning that this technique, typical for
predication, is sensitive to compiler optimizations. Depending on the
specific compiler and how the function is invoked, it is possible that a
branch may still be present or that suboptimal code may be generated.</p><p>In the case of Clang 16, this optimization performs well and provides a
significant improvement of 2.5-3x on small arrays:</p><img src=images/search-branchless.webp width=80% style=display:block;margin:auto><p>An interesting observation is that the branchless implementation
performs worse on large arrays, which might seem counterintuitive.
Considering that the total delay is primarily determined by RAM latency
and that both the branchless and branchy versions perform similar memory
accesses, one would expect their performance to be comparable or even
slightly better for the branchless version.</p><p>However, the key factor behind the better performance of the branchy
version lies in the CPU’s ability to speculate on one of the
branches. With branching, the CPU can start fetching either the left or
right key before confirming which one is the correct choice. This
speculative execution effectively acts as implicit <em><strong>prefetching</strong></em>,
reducing the impact of memory latency.</p><p>In contrast, the branchless implementation lacks this speculative
behavior since the <code>cmov</code> instruction is treated like any other
instruction, and the branch predictor does not attempt to predict its
operands to anticipate future execution. To compensate for this
limitation, we can introduce prefetching by explicitly requesting the
left and right child keys:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>auto</span> <span class=nf>lower_bound</span><span class=p>(</span><span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>array</span><span class=p>,</span> <span class=kt>int32_t</span> <span class=n>looking_for</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=o>-&gt;</span> <span class=kt>int32_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>current_index</span> <span class=o>=</span> <span class=mi>0U</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=n>size_t</span> <span class=n>length</span> <span class=o>=</span> <span class=n>array</span><span class=p>.</span><span class=n>size</span><span class=p>();</span> <span class=n>length</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=k>auto</span> <span class=n>half</span> <span class=o>=</span> <span class=n>length</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>length</span> <span class=o>-=</span> <span class=n>half</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>__builtin_prefetch</span><span class=p>(</span><span class=o>&amp;</span><span class=n>array</span><span class=p>[</span><span class=n>current_index</span> <span class=o>+</span> <span class=n>half</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=n>__builtin_prefetch</span><span class=p>(</span><span class=o>&amp;</span><span class=n>array</span><span class=p>[</span><span class=n>current_index</span> <span class=o>+</span> <span class=n>half</span> <span class=o>*</span> <span class=mi>2</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=n>current_index</span> <span class=o>+=</span> <span class=p>(</span><span class=n>array</span><span class=p>[</span><span class=n>current_index</span> <span class=o>+</span> <span class=n>half</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>looking_for</span><span class=p>)</span> <span class=o>*</span> <span class=n>half</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>array</span><span class=p>[</span><span class=n>current_index</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>And now, the performance becomes roughly the same:</p><img src=images/search-branchless-prefetch.webp width=80% style=display:block;margin:auto><p>Although the explicit prefetching in the branchless version helps
improve its performance on large arrays, the graph still shows a faster
growth compared to the branchy version. This is because the branchy
version takes advantage of speculative execution to prefetch not only
the immediate left or right child key but also
“grandchildren,” “great-grandchildren,” and so
on. Each additional speculative read, however, becomes exponentially
less likely to be correct, diminishing the usefulness of these
prefetches.</p><p>In the branchless version, it is possible to fetch ahead by more than
one layer, similar to the branchy version. However, the number of
fetches required would also grow exponentially, which is not a practical
solution. Therefore, an alternative approach is needed to optimize
memory operations.</p><h2 id=32-32-optimizing-the-layout>3.2 3.2. Optimizing the Layout<a hidden class=anchor aria-hidden=true href=#32-32-optimizing-the-layout>#</a></h2><p>Consider the following illustration:</p><img src=images/binary-search.webp width=80% style=display:block;margin:auto><p>The memory access pattern during binary search exhibits specific
characteristics that impact the likelihood of elements being cached and
the quality of their <em><strong>data locality</strong></em>. Let’s examine these
aspects in more detail:</p><ul><li><em><strong>Spatial locality</strong></em> seems relatively good for the last 3 to 4
requests, as they are likely to be within the same cache line.
However, for all the previous requests, there are significant memory
jumps, which can lead to poor spatial locality. This means that
accessing elements in those requests may result in cache misses,
requiring data to be fetched from main memory.</li><li><em><strong>Temporal Locality</strong></em> seems to be good for the first dozen of
requests. This is because there are not that many different comparison
sequences of this length, so the algorithm will repeatedly compare
against the same middle elements. These frequently accessed elements
are more likely to be cached.</li></ul><p>To further understand the importance of temporal locality and its impact
on cache performance, an experiment is conducted where the element to
compare is randomly chosen from within the search interval, rather than
using the middle one.</p><p>By introducing randomness, the temporal locality is disrupted, and each
comparison may involve different elements. This can lead to cache misses
and potentially degrade performance compared to the standard binary
search algorithm, where the middle element is consistently used for
comparison:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// gen_random(l, r) will generate random number in [l, r]
</span></span></span><span class=line><span class=cl><span class=c1>// use 32-bit Mersenne Twister and uniform random distribution
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>auto</span> <span class=nf>lower_bound</span><span class=p>(</span><span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>array</span><span class=p>,</span> <span class=kt>int32_t</span> <span class=n>looking_for</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=o>-&gt;</span> <span class=kt>int32_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>l</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>r</span> <span class=o>=</span> <span class=n>array</span><span class=p>.</span><span class=n>size</span><span class=p>()</span> <span class=o>-</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>while</span> <span class=p>(</span><span class=n>l</span> <span class=o>&lt;</span> <span class=n>r</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=n>size_t</span> <span class=n>m</span> <span class=o>=</span> <span class=n>gen_random</span><span class=p>(</span><span class=n>l</span><span class=p>,</span> <span class=n>r</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>array</span><span class=p>[</span><span class=n>m</span><span class=p>]</span> <span class=o>&gt;=</span> <span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>r</span> <span class=o>=</span> <span class=n>m</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span>
</span></span><span class=line><span class=cl>      <span class=n>l</span> <span class=o>=</span> <span class=n>m</span> <span class=o>+</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>array</span><span class=p>[</span><span class=n>l</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>Theoretically <a href=#ref-Gerald1984>[1]</a>, the randomized binary search is expected to do
30-40% more comparisons. However the benchmarking result shows ~6x on
large arrays:</p><img src=images/search-random.webp width=80% style=display:block;margin:auto><p>The degraded performance in the randomized selection experiment is not
solely due to the slow <code>gen_random()</code> call. The drop in performance can
be observed at the L2-L3 cache boundary, indicating that the memory
latency outweighs the overhead of random number generation and modulo
operations. In this case, the performance degradation occurs because the
fetched elements are unlikely to be cached, rather than just a small
suffix of them.</p><p>Another potential negative effect to consider is <em><strong>cache
associativity</strong></em>. If the array size is a multiple of a large power of
two, the indices of the frequently accessed elements may be divisible by
some large powers of two, resulting in cache conflicts. This can lead to
cache thrashing, where elements mapped to the same cache line repeatedly
evict each other, negatively impacting performance. For example, it
takes about ~400ns/query for searching over arrays of size $2^{20}$
while searching over arrays of size $(2^{20} + 123)$ takes ~300ns - a
20% difference.</p><p><a href=https://en.wikipedia.org/wiki/Fibonacci_search_technique>Fibonacci search</a> is one of the way to resolve this problem. However
the scope of this article will not talk about it, we are focusing on
temporal locality issue. By that, let’s ignore any cache side
effect by assuming all array sizes are $\lfloor 1.17^k \rfloor$ for
integer $k$.</p><p>The primary issue with the memory layout is that it does not effectively
leverage temporal locality. Specifically, the hot elements (frequently
accessed) and cold elements (infrequently accessed) are grouped together
in the same cache lines. For instance, the element
$\lfloor n/2 \rfloor$, which is requested on each query, is likely
stored in the same cache line as $\lfloor n/2 \rfloor + 1$, which is
rarely requested. This grouping of hot and cold elements hampers the
efficient utilization of temporal locality, leading to potential cache
misses and performance degradation.</p><p>Given the heatmap that visualize the frequency of comparisons for an
array of 31 elements:</p><img src=images/binary-heat.webp width=80% style=display:block;margin:auto><p>To optimize the memory layout and enhance cache efficiency, it is
desirable to group hot elements (frequently accessed) together and cold
elements (infrequently accessed) together.</p><p>To achieve this cache-friendly memory layout, I suggest renumbering the
elements using a 500 years-old numeration - The Eytzinger Layout.</p><h3 id=321-eytzinger-layout>3.2.1 Eytzinger Layout<a hidden class=anchor aria-hidden=true href=#321-eytzinger-layout>#</a></h3><p>According to Wiki:</p><blockquote><p><em><strong>Michaël Eytzinger</strong></em> was an Austrian nobleman, diplomat,
historian, and publicist, who wrote and published several works,
including a renowned volume that states the principles of a
genealogical numbering system, called an <em><strong>Ahnentafel</strong></em>, that is
still in use today.</p></blockquote><p>In the Ahnentafel (German for “ancestor table”) system, a
person’s direct ancestors are numbered in a fixed sequence of
ascent. The person themselves is assigned the number 1, and their
ancestors are numbered recursively. For each person with number $k$,
their father is assigned the number $2k$, and their mother is assigned
the number $(2k+1)$.</p><p>In computer science, this enumeration technique has found applications
in various data structures, particularly in implicit (pointer-free)
implementations of binary tree structures such as heaps and segment
trees. Instead of storing names, these structures use the Ahnentafel
numbering system to assign numbers to the underlying array items,
enabling efficient traversal and manipulation of the tree structure.</p><p>Consider how the layout looks in our example:</p><img src=images/eytzinger.webp width=80% style=display:block;margin:auto><p>With this layout, we are easily to start from the first element, then
jump to either $2 k$ or $(2k + 1)$:</p><img src=images/eytzinger-search.webp width=80% style=display:block;margin:auto><p>By reordering the elements of the array according to the Eytzinger
layout, we can exploit the inherent structure of the binary tree and
achieve optimal temporal locality. Elements closer to the root of the
tree, which correspond to smaller numbers in the ahnentafel numbering,
are placed closer to the beginning of the array. As a result, they are
more likely to be fetched from the cache, leading to improved
performance.</p><img src=images/eytzinger-heat.webp width=80% style=display:block;margin:auto><p>The process of reordering the elements using the Eytzinger layout
follows a specific pattern. Even-indexed elements are written to the end
of the new array, while the remaining elements are written in a similar
manner, preserving their relative order within each level of the binary
tree. The first element will be placed as the root of the tree.</p><h4 id=3211-building-the-layout>3.2.1.1 Building the layout<a hidden class=anchor aria-hidden=true href=#3211-building-the-layout>#</a></h4><p>To construct the layout, we can follow an article from <a href=https://codeforces.com/blog/entry/75421>Codeforces</a>, in
$O(\log n)$ times:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// array: original sorted array
</span></span></span><span class=line><span class=cl><span class=c1>// eytzinger_array: 1-indexed array
</span></span></span><span class=line><span class=cl><span class=c1>// array_index will start from 0
</span></span></span><span class=line><span class=cl><span class=c1>// eytzinger_index will start from 1 (named as |k| in formula)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>auto</span> <span class=nf>build_eytzinger_array</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>array</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>eytzinger_array</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>array_index</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>eytzinger_index</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>size_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>eytzinger_index</span> <span class=o>&lt;=</span> <span class=n>array</span><span class=p>.</span><span class=n>size</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=k>auto</span> <span class=n>eytzinger_index_x2</span> <span class=o>=</span> <span class=n>eytzinger_index</span> <span class=o>*</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>array_index</span>                   <span class=o>=</span> <span class=n>build_eytzinger_array</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=n>array</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>eytzinger_array</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>array_index</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>eytzinger_index_x2</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>eytzinger_array</span><span class=p>[</span><span class=n>eytzinger_index</span><span class=p>]</span> <span class=o>=</span> <span class=n>array</span><span class=p>[</span><span class=n>array_index</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=o>++</span><span class=n>array_index</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>array_index</span> <span class=o>=</span> <span class=n>build_eytzinger_array</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=n>array</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>eytzinger_array</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>array_index</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>eytzinger_index_x2</span> <span class=o>+</span> <span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>array_index</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>This function takes the current node number <code>eytzinger_index</code> and
recursively constructs the Eytzinger array. It writes out the elements
to the left of the middle of the search interval, then adds the current
element for comparison, and finally writes out the elements to the
right. Although it may appear complex at first glance, a few
observations can help understand its functionality:</p><ul><li>It writes exactly <code>array.size()</code> elements, as the <code>if</code> block is
entered once for each <code>eytzinger_index</code> ranging from <code>1</code> to
<code>array.size()</code>.</li><li>The elements from the original array are written sequentially,
incrementing the <code>array_index</code> with each iteration.</li><li>By the time the element at node <code>eytzinger_index</code> is written, all the
elements to its left (up to <code>array_index</code>) have already been written.</li></ul><p>Despite its recursive nature, the function is quite efficient due to the
sequential memory reads and writes ($O(\log n)$ at a time). However,
maintaining the permutation can be more challenging both conceptually
and computationally. Unlike adding an element to a sorted array, which
involves shifting a suffix of elements, the Eytzinger array needs to be
rebuilt entirely.</p><p>Although the new layout may not be exactly equivalent to the
“tree” structure of a standard binary search (For instance,
the left child subtree may have a larger size than the right child
subtree, potentially up to twice as large), this discrepancy does not
significantly impact the overall performance, as both approaches still
result in a binary search tree with a depth of approximately
$\lceil \log_2 n \rceil$.</p><p>Additionally, it’s worth mentioning that the Eytzinger array is
1-indexed, which holds importance for optimizing performance later. You
can assign a value to the 0-th element to indicate the desired return
value when the lower bound does not exist, like using <code>array.end()</code> for
<code>std::lower_bound</code>.</p><h4 id=3212-implement-the-searching>3.2.1.2 Implement the Searching<a hidden class=anchor aria-hidden=true href=#3212-implement-the-searching>#</a></h4><p>Now, with the Eytzinger array and its optimized traversal, we can
descend the array using only indices, eliminating the need to store and
recalculate search boundaries. This efficient approach also allows us to
avoid branching in our algorithm. Here’s the code snippet:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// named as |k| in formula
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>size_t</span> <span class=n>current_eytzinger_index</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=p>(</span><span class=n>current_eytzinger_index</span> <span class=o>&lt;=</span> <span class=n>array</span><span class=p>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl>  <span class=n>current_eytzinger_index</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=mi>2</span> <span class=o>*</span> <span class=n>current_eytzinger_index</span> <span class=o>+</span> <span class=p>(</span><span class=n>eytzinger_array</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>looking_for</span><span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><p>By starting with $k=1$ and updating it as
$k = 2 * k + (eytzinger\_array[k] &lt; looking\_for)$, we can navigate
through the array efficiently, moving left if the current element is
greater than or equal to $looking_for$, and right otherwise. The
simplicity and lack of branching in this loop contribute to improved
performance.</p><p>However, a challenge arises when we need to determine the index of the
resulting element, as $k$ does not directly point to it. Let’s
consider an example to understand this issue (corresponding tree
provided above):</p><pre class=center-pre>
    array:  0 1 2 3 4 5 6 7 8 9
eytzinger:  <u>6</u> <u>3</u> 7 <u>1</u> 5 8 9 0 <u>2</u> 4
1st range:  ------------?------  k := 2*k     = 2   (6 >= 3)
2nd range:  ------?------        k := 2*k     = 4   (3 >= 3)
3rd range:  --?----              k := 2*k + 1 = 9   (1 < 3)
4th range:      ?--              k := 2*k + 1 = 19  (2 < 3)
5th range:        !
</pre><p>In this example, we query the array of $[0, &mldr;, 9]$ for the lower bound
of $x=3$. We compare $x$ against $[6, 3, 1, 2]$, proceeding with
left-left-right-right steps until we end up with $k=19$. However, $k=19$
is not a valid array index.</p><p>By observing the flow, we can recognize that we can compare $x$ against
it at some point unless the answer is the end of the array. Once we
determine that the answer is not less than $x$, we make a left turn and
then continue going right until we reach a leaf (as we only compare $x$
against lesser elements). Therefore, to restore the correct answer,
“cancel” some right turns and then make one additional move.</p><p>A solution is to recognize that the right turns are recorded as 1-bits
in the binary representation of $k$. Therefore, finding the number of
trailing 1-bits then right-shifting $k$ by that number plus one is the
way to restore the answer. This can be achieved by inverting the number
(<code>~k</code>) and letting the <code>__builtin_ffs</code> (or its variants) come to play:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>auto</span> <span class=nf>eytzinger_search</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>eytzinger_array</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=kt>int32_t</span> <span class=n>looking_for</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kt>int32_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=c1>// named as |k| in formulas
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>size_t</span> <span class=n>current_index</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>while</span> <span class=p>(</span><span class=n>current_index</span> <span class=o>&lt;</span> <span class=n>eytzinger_array</span><span class=p>.</span><span class=n>size</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>current_index</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>      <span class=p>(</span><span class=n>current_index</span> <span class=o>*</span> <span class=mi>2</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>eytzinger_array</span><span class=p>[</span><span class=n>current_index</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>looking_for</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=n>current_index</span> <span class=o>&gt;&gt;=</span> <span class=n>__builtin_ffs</span><span class=p>(</span><span class=o>~</span><span class=n>current_index</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>eytzinger_array</span><span class=p>[</span><span class=n>current_index</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>Unfortunately, the result is quite sad:</p><img src=images/search-eytzinger.webp width=80% style=display:block;margin:auto><p>When comparing the latency of smaller arrays, the branchless binary
search implementation and the Eytzinger Search exhibit similar
performance. This similarity is expected since the branchless
implementation is concise and efficient. However, as the array size
increases, the performance of the Eytzinger Search lose the run.</p><p>The reason behind this performance difference lies in the loss of
spatial locality in the Eytzinger Search. In the branchless
implementation, the last 3-4 elements being compared are likely to
reside in the same cache line, allowing for efficient memory access.
However, in the Eytzinger layout, these last elements are no longer
grouped together in a single cache line, resulting in separate memory
fetches for each element.</p><p>One might argue that the improved temporal locality in the Eytzinger
Search should compensate for the loss of spatial locality. Previously,
only a fraction (approximately $\frac{1}{16}$) of the cache line was
utilized to store a hot element, whereas now the entire cache line is
used, effectively increasing the effective cache size by a factor of 16.
This expanded cache size should enable covering approximately 4 more
initial requests based on logarithmic calculations.</p><p>However, upon closer examination, it becomes apparent that this
compensation is not sufficient. Caching the other 15 elements in the
cache line was not entirely useless, and the hardware could have fetched
neighboring cache lines in advance. If the last requests in the sequence
were performed, the subsequent memory reads would likely involve
accessing cached elements. Therefore, in reality, it is more likely that
the last 6-7 accesses would benefit from caching, rather than just 3-4.</p><p>At first glance, it may seem that adopting the Eytzinger layout was an
imprudent decision. However, let’s spend our efforts to make it
worthwhile.</p><h4 id=3213-on-trying-to-prefetch>3.2.1.3 On trying to prefetch<a hidden class=anchor aria-hidden=true href=#3213-on-trying-to-prefetch>#</a></h4><p>To mitigate the impact of memory latency in the Eytzinger Search, we can
employ prefetching techniques similar to those used in the branchless
binary search. However, in the case of this layout, we can optimize the
prefetching process by leveraging the sequential nature of neighboring
nodes in the array.</p><p>In the Eytzinger array, the left ($2k$) and right child nodes
($(2k + 1)$) of a given node, are likely to reside in the same cache
line. This observation allows us to issue a single prefetch instruction
for both child nodes, reducing the overhead associated with separate
prefetches.</p><p>Let extends this observation further to the grand-children of node $k$,
which are stored sequentially in the array. By examining the indices, we
can deduce that the cache line containing these grand-children can also
be fetched using a single instruction. Specifically, the indices $4k$,
$4k + 1$, $4k + 2$, and $4k + 3$ correspond to the great-grandchildren
of node $k$.</p><p>We can continue this pattern and prefetch even more descendants in
advance. By fetching ahead as many as possible within a single cache
line, we can prefetch up to $\frac{64}{4} = 16$ elements, which are the
great-great-grandchildren of a node $k$, spanning indices from $16k$ to
$(16k + 15)$.</p><p>To ensure that we retrieve all $16$ elements with a single memory
request, we need to take cache line boundaries into consideration. By
prefetching both the first and last element of this group, we increase
the chance of capturing all of them. Crucially, we can exploit the fact
that the index of the first element, $16k$, is divisible by $16$.
Consequently, the memory address of this element will be the base
address of the array plus a value divisible by $64$, which is also the
cache line size.</p><p>Thus, if the array starts on a cache line boundary, we can guarantee
that these $16$ elements will reside entirely within a single cache
line.</p><p>Therefore, why not align the array by a magic $64$?</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// 1-indexed
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>alignas</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;</span> <span class=n>eytzinger_array</span><span class=p>(</span><span class=n>array</span><span class=p>.</span><span class=n>size</span><span class=p>()</span> <span class=o>+</span> <span class=mi>1</span><span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><p>And then prefetch the first in the group of $16k$ on every iteration:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>auto</span> <span class=nf>eytzinger_search</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int32_t</span><span class=o>&gt;&amp;</span> <span class=n>eytzinger_array</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=kt>int32_t</span> <span class=n>looking_for</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kt>int32_t</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=c1>// named as |k| in formulas
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>size_t</span> <span class=n>current_index</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>while</span> <span class=p>(</span><span class=n>current_index</span> <span class=o>&lt;</span> <span class=n>eytzinger_array</span><span class=p>.</span><span class=n>size</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>__builtin_prefetch</span><span class=p>(</span><span class=n>eytzinger_array</span><span class=p>.</span><span class=n>data</span><span class=p>()</span> <span class=o>+</span> <span class=p>(</span><span class=n>current_index</span> <span class=o>*</span> <span class=mi>16</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=n>current_index</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>      <span class=p>(</span><span class=n>current_index</span> <span class=o>*</span> <span class=mi>2</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>eytzinger_array</span><span class=p>[</span><span class=n>current_index</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>looking_for</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=n>current_index</span> <span class=o>&gt;&gt;=</span> <span class=n>__builtin_ffs</span><span class=p>(</span><span class=o>~</span><span class=n>current_index</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>eytzinger_array</span><span class=p>[</span><span class=n>current_index</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>And here is the result:</p><img src=images/search-eytzinger-prefetch.webp width=80% style=display:block;margin:auto><p>The performance improvement achieved by the prefetching technique in
large arrays is quite significant, approximately 3-4 times faster
compared to the previous version and around 2 times faster than
<code>std::lower_bound</code>. This enhancement effectively hides memory latency by
prefetching data four steps ahead and overlapping memory requests.</p><p>In theory, if computation was not a limiting factor, we would expect a
speedup of approximately 4 times. However, in practice, the speedup is
somewhat more moderate due to various factors.</p><p>Furthermore, it’s worth considering prefetching beyond the 4 steps
mentioned earlier. We can attempt to request only the first cache line
and rely on the hardware to prefetch its neighboring cache lines.
It’s important to note that <em><strong>the effectiveness of this trick
depends on the specific hardware in use, and it may or may not improve
overall performance</strong></em>.</p><p>Additionally, one should bear in mind that the prefetch requests may not
be necessary (typically the last prefetch) and could potentially be
invalid (out of the program allocated memory). Although modern CPUs
treat these invalid instructions as no-ops (which means there are no
slow-down problems), one should unroll the last few iterations out of
the loop to eliminate unnecessary prefetches.</p><p>Finally, prefetching does come at a cost. It effectively trades excess
memory bandwidth for reduced latency. Therefore, for instance, if you
run multiple instances simultaneously on separate hardware threads, it
can significantly impact the benchmark performance.</p><h1 id=4-references>4 References<a hidden class=anchor aria-hidden=true href=#4-references>#</a></h1><div id=refs class="references csl-bib-body"><div id=ref-Gerald1984 class=csl-entry><p><span class=csl-left-margin>[1]
</span><span class=csl-right-inline>G. G. Brown and B. O. Shubert,
“On random binary trees,” <em>Mathematics of Operations
Research</em>, vol. 9, no. 1, pp. 43–65, 1984, Accessed: Jun. 13,
2023. [Online]. Available:
<a href=http://www.jstor.org/stable/3689501>http://www.jstor.org/stable/3689501</a></span></p></div><div id=ref-khuong2017 class=csl-entry><p><span class=csl-left-margin>[2]
</span><span class=csl-right-inline>P.-V. Khuong and P. Morin,
“Array layouts for comparison-based searching,” <em>ACM J. Exp.
Algorithmics</em>, vol. 22, May 2017, doi: <a href=https://doi.org/10.1145/3053370>10.1145/3053370</a>.</span></p></div><div id=ref-Malte2023 class=csl-entry><p><span class=csl-left-margin>[3]
</span><span class=csl-right-inline>M. Skarupke, “Beautiful
branchless binary search.” PROBABLY DANCE, 2023.Available:
<a href=https://probablydance.com/2023/04/27/beautiful-branchless-binary-search/>https://probablydance.com/2023/04/27/beautiful-branchless-binary-search/</a></span></p></div></div></div><hr><footer class=post-footer><ul class=post-tags><li><a href=https://philong6297.github.io/tags/cpp/>cpp</a></li></ul><nav class=paginav><a class=next href=https://philong6297.github.io/posts/cpp_parallel_stl_is_not_yet_for_indiscriminate_use/><span class=title>Next »</span><br><span>C++ Parallel STL is not yet for indiscriminate use</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 Phi-Long Le</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>